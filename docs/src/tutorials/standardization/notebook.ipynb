{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Effects of Feature Standardization on Model Performance\n",
    "\n",
    "Welcome to this tutorial on feature standardization in machine learning!\n",
    "In this tutorial, we'll explore how standardizing features can significantly\n",
    "impact the performance of different machine learning models.\n",
    "\n",
    "We'll compare Logistic Regression and Support Vector Machine (SVM) models,\n",
    "both with and without feature standardization. This will help us understand\n",
    "when and why preprocessing is important for model performance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "First, let's make sure we're using a compatible Julia version. This code was tested with Julia 1.10.\n",
    "Let's import all the packages we'll need for this tutorial."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Load the necessary packages\n",
    "using MLJ                   # Core MLJ framework\n",
    "using LIBSVM                # For Support Vector Machine\n",
    "using DataFrames            # For displaying results\n",
    "using RDatasets             # To load sample datasets\n",
    "using Random                # For reproducibility\n",
    "using ScientificTypes       # For proper data typing\n",
    "using Plots                 # For visualizations\n",
    "using MLJLinearModels       # For Logistic Regression"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation\n",
    "\n",
    "Let's load the Pima Indians Diabetes Dataset. This is a classic dataset for\n",
    "binary classification, where we predict diabetes status based on various health metrics.\n",
    "\n",
    "The interesting thing about this dataset is that different features have very different scales.\n",
    "We'll artificially exaggerate this by adding a large constant to the glucose values."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Load the dataset and modify it to have extreme scale differences\n",
    "df = RDatasets.dataset(\"MASS\", \"Pima.tr\")\n",
    "df.Glu .+= 10000.0;  # Artificially increase the scale of glucose values"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's examine the first few rows of our dataset:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\u001b[1m5×8 DataFrame\u001b[0m\n\u001b[1m Row \u001b[0m│\u001b[1m NPreg \u001b[0m\u001b[1m Glu     \u001b[0m\u001b[1m BP    \u001b[0m\u001b[1m Skin  \u001b[0m\u001b[1m BMI     \u001b[0m\u001b[1m Ped     \u001b[0m\u001b[1m Age   \u001b[0m\u001b[1m Type \u001b[0m\n     │\u001b[90m Int32 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int32 \u001b[0m\u001b[90m Int32 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int32 \u001b[0m\u001b[90m Cat… \u001b[0m\n─────┼─────────────────────────────────────────────────────────────\n   1 │     5  10086.0     68     28     30.2    0.364     24  No\n   2 │     7  10195.0     70     33     25.1    0.163     55  Yes\n   3 │     5  10077.0     82     41     35.8    0.156     35  No\n   4 │     0  10165.0     76     43     47.9    0.259     26  No\n   5 │     0  10107.0     60     25     26.4    0.133     23  No",
      "text/html": [
       "<div><div style = \"float: left;\"><span>5×8 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">NPreg</th><th style = \"text-align: left;\">Glu</th><th style = \"text-align: left;\">BP</th><th style = \"text-align: left;\">Skin</th><th style = \"text-align: left;\">BMI</th><th style = \"text-align: left;\">Ped</th><th style = \"text-align: left;\">Age</th><th style = \"text-align: left;\">Type</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Int32\" style = \"text-align: left;\">Int32</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int32\" style = \"text-align: left;\">Int32</th><th title = \"Int32\" style = \"text-align: left;\">Int32</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int32\" style = \"text-align: left;\">Int32</th><th title = \"CategoricalArrays.CategoricalValue{String, UInt8}\" style = \"text-align: left;\">Cat…</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">10086.0</td><td style = \"text-align: right;\">68</td><td style = \"text-align: right;\">28</td><td style = \"text-align: right;\">30.2</td><td style = \"text-align: right;\">0.364</td><td style = \"text-align: right;\">24</td><td style = \"text-align: left;\">No</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">10195.0</td><td style = \"text-align: right;\">70</td><td style = \"text-align: right;\">33</td><td style = \"text-align: right;\">25.1</td><td style = \"text-align: right;\">0.163</td><td style = \"text-align: right;\">55</td><td style = \"text-align: left;\">Yes</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">10077.0</td><td style = \"text-align: right;\">82</td><td style = \"text-align: right;\">41</td><td style = \"text-align: right;\">35.8</td><td style = \"text-align: right;\">0.156</td><td style = \"text-align: right;\">35</td><td style = \"text-align: left;\">No</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">10165.0</td><td style = \"text-align: right;\">76</td><td style = \"text-align: right;\">43</td><td style = \"text-align: right;\">47.9</td><td style = \"text-align: right;\">0.259</td><td style = \"text-align: right;\">26</td><td style = \"text-align: left;\">No</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">10107.0</td><td style = \"text-align: right;\">60</td><td style = \"text-align: right;\">25</td><td style = \"text-align: right;\">26.4</td><td style = \"text-align: right;\">0.133</td><td style = \"text-align: right;\">23</td><td style = \"text-align: left;\">No</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "first(df, 5)"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Type Conversion\n",
    "\n",
    "In MLJ, it's important to ensure that our data has the correct scientific types.\n",
    "This helps the framework understand how to properly handle each column.\n",
    "\n",
    "We'll convert our columns to their appropriate types:\n",
    "- `Count` for discrete count data\n",
    "- `Continuous` for continuous numerical data\n",
    "- `Multiclass` for our target variable"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Coerce columns to the right scientific types\n",
    "df = coerce(df,\n",
    "    :NPreg => Count,      # Number of pregnancies is a count\n",
    "    :Glu => Continuous,   # Glucose level is continuous\n",
    "    :BP => Continuous,    # Blood pressure is continuous\n",
    "    :Skin => Continuous,  # Skin thickness is continuous\n",
    "    :BMI => Continuous,   # Body mass index is continuous\n",
    "    :Ped => Continuous,   # Diabetes pedigree is continuous\n",
    "    :Age => Continuous,   # Age is continuous\n",
    "    :Type => Multiclass,  # Diabetes status is our target (Yes/No)\n",
    ");"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's verify that our schema looks correct:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌───────┬───────────────┬─────────────────────────────────┐\n│\u001b[22m names \u001b[0m│\u001b[22m scitypes      \u001b[0m│\u001b[22m types                           \u001b[0m│\n├───────┼───────────────┼─────────────────────────────────┤\n│ NPreg │ Count         │ Int32                           │\n│ Glu   │ Continuous    │ Float64                         │\n│ BP    │ Continuous    │ Float64                         │\n│ Skin  │ Continuous    │ Float64                         │\n│ BMI   │ Continuous    │ Float64                         │\n│ Ped   │ Continuous    │ Float64                         │\n│ Age   │ Continuous    │ Float64                         │\n│ Type  │ Multiclass{2} │ CategoricalValue{String, UInt8} │\n└───────┴───────────────┴─────────────────────────────────┘\n"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "ScientificTypes.schema(df)"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Extraction and Data Splitting\n",
    "\n",
    "Now we'll separate our features from our target variable.\n",
    "In MLJ, this is done with the `unpack` function."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Unpack features (X) and target (y)\n",
    "y, X = unpack(df, ==(:Type); rng = 123);"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll split our data into training and testing sets.\n",
    "We'll use 70% for training and 30% for testing."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Split data into train and test sets\n",
    "train, test = partition(eachindex(y), 0.7, shuffle = true, rng = 123);"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Setup\n",
    "\n",
    "We'll compare two different models:\n",
    "1. Logistic Regression: A linear model good for binary classification\n",
    "2. Support Vector Machine (SVM): A powerful non-linear classifier\n",
    "\n",
    "For each model, we'll create two versions:\n",
    "- One without standardization\n",
    "- One with standardization\n",
    "\n",
    "The `Standardizer` transformer will rescale our features to have mean 0 and standard deviation 1."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import MLJLinearModels ✔\n",
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import MLJLIBSVMInterface ✔\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DeterministicPipeline(\n  standardizer = Standardizer(\n        features = Symbol[], \n        ignore = false, \n        ordered_factor = false, \n        count = false), \n  svc = SVC(\n        kernel = LIBSVM.Kernel.RadialBasis, \n        gamma = 0.0, \n        cost = 1.0, \n        cachesize = 200.0, \n        degree = 3, \n        coef0 = 0.0, \n        tolerance = 0.001, \n        shrinking = true), \n  cache = true)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "cell_type": "code",
   "source": [
    "# Load our models from their respective packages\n",
    "logreg = @load LogisticClassifier pkg = MLJLinearModels\n",
    "svm = @load SVC pkg = LIBSVM\n",
    "stand = Standardizer()  # This is our standardization transformer\n",
    "\n",
    "# Create pipelines for each model variant\n",
    "logreg_pipe = logreg()  # Plain logistic regression\n",
    "logreg_std_pipe = Pipeline(stand, logreg())  # Logistic regression with standardization\n",
    "svm_pipe = svm()  # Plain SVM\n",
    "svm_std_pipe = Pipeline(stand, svm())  # SVM with standardization"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Let's set up a vector of our models so we can evaluate them all using the same process.\n",
    "For each model, we'll store its name and the corresponding pipeline."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "4-element Vector{Tuple{String, MLJModelInterface.Supervised}}:\n (\"Logistic Regression\", LogisticClassifier(lambda = 2.220446049250313e-16, …))\n (\"Logistic Regression (standardized)\", ProbabilisticPipeline(standardizer = Standardizer(features = Symbol[], …), …))\n (\"SVM\", SVC(kernel = RadialBasis, …))\n (\"SVM (standardized)\", DeterministicPipeline(standardizer = Standardizer(features = Symbol[], …), …))"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "cell_type": "code",
   "source": [
    "# Create a list of models to evaluate\n",
    "models = [\n",
    "    (\"Logistic Regression\", logreg_pipe),\n",
    "    (\"Logistic Regression (standardized)\", logreg_std_pipe),\n",
    "    (\"SVM\", svm_pipe),\n",
    "    (\"SVM (standardized)\", svm_std_pipe),\n",
    "]"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we'll loop through each model, train it, make predictions, and calculate accuracy.\n",
    "This will help us compare how standardization affects each model's performance."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌ Warning: The number and/or types of data arguments do not match what the specified model\n",
      "│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n",
      "│ \n",
      "│ Run `@doc MLJLinearModels.LogisticClassifier` to learn more about your model's requirements.\n",
      "│ \n",
      "│ Commonly, but non exclusively, supervised models are constructed using the syntax\n",
      "│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n",
      "│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n",
      "│ sample or class weights.\n",
      "│ \n",
      "│ In general, data in `machine(model, data...)` is expected to satisfy\n",
      "│ \n",
      "│     scitype(data) <: MLJ.fit_data_scitype(model)\n",
      "│ \n",
      "│ In the present case:\n",
      "│ \n",
      "│ scitype(data) = Tuple{ScientificTypesBase.Table{Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Count}}}, AbstractVector{ScientificTypesBase.Multiclass{2}}}\n",
      "│ \n",
      "│ fit_data_scitype(model) = Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}\n",
      "└ @ MLJBase ~/.julia/packages/MLJBase/7nGJF/src/machines.jl:237\n",
      "[ Info: Training machine(LogisticClassifier(lambda = 2.220446049250313e-16, …), …).\n",
      "┌ Info: Solver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n",
      "│   optim_options: Optim.Options{Float64, Nothing}\n",
      "└   lbfgs_options: @NamedTuple{} NamedTuple()\n",
      "[ Info: Training machine(ProbabilisticPipeline(standardizer = Standardizer(features = Symbol[], …), …), …).\n",
      "[ Info: Training machine(:standardizer, …).\n",
      "[ Info: Training machine(:logistic_classifier, …).\n",
      "┌ Info: Solver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n",
      "│   optim_options: Optim.Options{Float64, Nothing}\n",
      "└   lbfgs_options: @NamedTuple{} NamedTuple()\n",
      "┌ Warning: The number and/or types of data arguments do not match what the specified model\n",
      "│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n",
      "│ \n",
      "│ Run `@doc LIBSVM.SVC` to learn more about your model's requirements.\n",
      "│ \n",
      "│ Commonly, but non exclusively, supervised models are constructed using the syntax\n",
      "│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n",
      "│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n",
      "│ sample or class weights.\n",
      "│ \n",
      "│ In general, data in `machine(model, data...)` is expected to satisfy\n",
      "│ \n",
      "│     scitype(data) <: MLJ.fit_data_scitype(model)\n",
      "│ \n",
      "│ In the present case:\n",
      "│ \n",
      "│ scitype(data) = Tuple{ScientificTypesBase.Table{Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Count}}}, AbstractVector{ScientificTypesBase.Multiclass{2}}}\n",
      "│ \n",
      "│ fit_data_scitype(model) = Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}, Any}}\n",
      "└ @ MLJBase ~/.julia/packages/MLJBase/7nGJF/src/machines.jl:237\n",
      "[ Info: Training machine(SVC(kernel = RadialBasis, …), …).\n",
      "[ Info: Training machine(DeterministicPipeline(standardizer = Standardizer(features = Symbol[], …), …), …).\n",
      "[ Info: Training machine(:standardizer, …).\n",
      "[ Info: Training machine(:svc, …).\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "# Train and evaluate each model\n",
    "results = DataFrame(model = String[], accuracy = Float64[])\n",
    "for (name, model) in models\n",
    "    # Create a machine learning model\n",
    "    mach = machine(model, X, y)\n",
    "\n",
    "    # Train the model on the training data\n",
    "    MLJ.fit!(mach, rows = train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    # Note: Logistic regression returns probabilities, so we need to get the mode\n",
    "    yhat =\n",
    "        occursin(\"Logistic Regression\", name) ?\n",
    "        MLJ.predict_mode(mach, rows = test) :  # Get most likely class for logistic regression\n",
    "        MLJ.predict(mach, rows = test)         # SVM directly predicts the class\n",
    "\n",
    "    # Calculate accuracy\n",
    "    acc = accuracy(yhat, y[test])\n",
    "\n",
    "    # Store the results\n",
    "    push!(results, (name, acc))\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results Visualization\n",
    "\n",
    "Finally, let's visualize our results to see the impact of standardization.\n",
    "We'll create a bar chart comparing the accuracy of each model."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Create a bar chart of model performance\n",
    "p = bar(\n",
    "    results.model,\n",
    "    results.accuracy,\n",
    "    xlabel = \"Model\",\n",
    "    ylabel = \"Accuracy\",\n",
    "    title = \"Model Accuracy Comparison\",\n",
    "    legend = false,\n",
    "    bar_width = 0.6,\n",
    "    ylims = (0.5, 0.7),\n",
    "    xrotation = 17,\n",
    ");"
   ],
   "metadata": {},
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the plot"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "savefig(p, \"standardization_results.png\"); #hide"
   ],
   "metadata": {},
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "From this tutorial, we can clearly see that standardization has a dramatic impact on model performance.\n",
    "\n",
    "Looking at the results:\n",
    "\n",
    "- **Logistic Regression**: Without standardization, it achieves only ~57% accuracy. With standardization,\n",
    "  its performance jumps dramatically to ~68% accuracy – the best performance among all models.\n",
    "\n",
    "- **SVM**: The baseline SVM achieves ~62% accuracy. When standardized, it improves to ~65% accuracy,\n",
    "  which is a significant boost but not as dramatic as what we see with logistic regression.\n",
    "\n",
    "Try this approach with other datasets and models to further explore the effects of standardization!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  },
  "kernelspec": {
   "name": "julia-1.11",
   "display_name": "Julia 1.11.5",
   "language": "julia"
  }
 },
 "nbformat": 4
}
