var documenterSearchIndex = {"docs":
[{"location":"transformers/neural/","page":"Neural-based Encoders","title":"Neural-based Encoders","text":"Neural-based Encoders include categorical encoders based on neural networks:","category":"page"},{"location":"transformers/neural/","page":"Neural-based Encoders","title":"Neural-based Encoders","text":"Transformer Brief Description\nEntityEmbedder Encode categorical variables into dense embedding vectors","category":"page"},{"location":"transformers/neural/","page":"Neural-based Encoders","title":"Neural-based Encoders","text":"This method has been implemented in the MLJFlux.jl which is a package that provides MLJ interfaces to deep learning models from Flux.jl. The EntityEmbedder encodes categorical features into dense vectors using a deep learning model that we must supply such as MLJFlux.NeuralNetworkClassifier or MLJFlux.NeuralNetworkRegressor. See the full docstring for more information:","category":"page"},{"location":"transformers/neural/","page":"Neural-based Encoders","title":"Neural-based Encoders","text":"MLJFlux.EntityEmbedder","category":"page"},{"location":"transformers/utility/","page":"Utility Encoders","title":"Utility Encoders","text":"Utility Encoders include categorical encoders meant to be used as preprocessors for other encoders or models.","category":"page"},{"location":"transformers/utility/","page":"Utility Encoders","title":"Utility Encoders","text":"Transformer Brief Description\nCardinalityReducer Reduce cardinality of high cardinality categorical features by grouping infrequent categories\nMissingnessEncoder Encode missing values of categorical features into new values","category":"page"},{"location":"transformers/utility/","page":"Utility Encoders","title":"Utility Encoders","text":"MLJTransforms.CardinalityReducer","category":"page"},{"location":"transformers/utility/#MLJTransforms.CardinalityReducer","page":"Utility Encoders","title":"MLJTransforms.CardinalityReducer","text":"CardinalityReducer\n\nA model type for constructing a cardinality reducer, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nCardinalityReducer = @load CardinalityReducer pkg=MLJTransforms\n\nDo model = CardinalityReducer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in CardinalityReducer(features=...).\n\nCardinalityReducer maps any level of a categorical feature that occurs with frequency < min_frequency into a new level (e.g., \"Other\"). This is useful when some categorical features have high cardinality and many levels are infrequent. This assumes that the categorical features have raw types that are in Union{AbstractString, Char, Number}.\n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to   check scitypes. \n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\nmin_frequency::Real=3: Any level of a categorical feature that occurs with frequency < min_frequency will be mapped to a new level. Could be\n\nan integer or a float which decides whether raw counts or normalized frequencies are used.\n\nlabel_for_infrequent::Dict{<:Type, <:Any}()= Dict( AbstractString => \"Other\", Char => 'O', ): A\n\ndictionary where the possible values for keys are the types in Char, AbstractString, and Number and each value signifies the new level to map into given a column raw super type. By default, if the raw type of the column subtypes AbstractString then the new value is \"Other\" and if the raw type subtypes Char then the new value is 'O' and if the raw type subtypes Number then the new value is the lowest value in the column - 1.\n\nOperations\n\ntransform(mach, Xnew): Apply cardinality reduction to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and   return the new table.   Features that are neither Multiclass nor OrderedFactor  are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nnew_cat_given_col_val: A dictionary that maps each level in a   categorical feature to a new level (either itself or the new level specified in label_for_infrequent)\n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nimport StatsBase.proportionmap\nusing MLJ\n\n# Define categorical features\nA = [ [\"a\" for i in 1:100]..., \"b\", \"b\", \"b\", \"c\", \"d\"]\nB = [ [0 for i in 1:100]..., 1, 2, 3, 4, 4]\n\n# Combine into a named tuple\nX = (A = A, B = B)\n\n# Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Multiclass\n)\n\nencoder = CardinalityReducer(ordered_factor = false, min_frequency=3)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia> proportionmap(Xnew.A)\nDict{CategoricalArrays.CategoricalValue{String, UInt32}, Float64} with 3 entries:\n  \"Other\" => 0.0190476\n  \"b\"     => 0.0285714\n  \"a\"     => 0.952381\n\njulia> proportionmap(Xnew.B)\nDict{CategoricalArrays.CategoricalValue{Int64, UInt32}, Float64} with 2 entries:\n  0  => 0.952381\n  -1 => 0.047619\n\nSee also FrequencyEncoder\n\n\n\n\n\n","category":"type"},{"location":"transformers/utility/","page":"Utility Encoders","title":"Utility Encoders","text":"MLJTransforms.MissingnessEncoder","category":"page"},{"location":"transformers/utility/#MLJTransforms.MissingnessEncoder","page":"Utility Encoders","title":"MLJTransforms.MissingnessEncoder","text":"MissingnessEncoder\n\nA model type for constructing a missingness encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMissingnessEncoder = @load MissingnessEncoder pkg=MLJTransforms\n\nDo model = MissingnessEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MissingnessEncoder(features=...).\n\nMissingnessEncoder maps any missing level of a categorical feature into a new level (e.g., \"Missing\").  By this, missingness will be treated as a new level by any subsequent model. This assumes that the categorical features have raw types that are in Char, AbstractString, and Number.\n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to   check scitypes. \n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\nlabel_for_missing::Dict{<:Type, <:Any}()= Dict( AbstractString => \"missing\", Char => 'm', ): A\n\ndictionary where the possible values for keys are the types in Char, AbstractString, and Number and where each value signifies the new level to map into given a column raw super type. By default, if the raw type of the column subtypes AbstractString then missing values will be replaced with \"missing\" and if the raw type subtypes Char then the new value is 'm' and if the raw type subtypes Number then the new value is the lowest value in the column - 1.\n\nOperations\n\ntransform(mach, Xnew): Apply cardinality reduction to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and   return the new table.   Features that are neither Multiclass nor OrderedFactor  are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nlabel_for_missing_given_feature: A dictionary that for each column, maps missing into some value according to label_for_missing\n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nimport StatsBase.proportionmap\nusing MLJ\n\n# Define a table with missing values\nXm = (\n    A = categorical([\"Ben\", \"John\", missing, missing, \"Mary\", \"John\", missing]),\n    B = [1.85, 1.67, missing, missing, 1.5, 1.67, missing],\n    C= categorical([7, 5, missing, missing, 10, 0, missing]),\n    D = [23, 23, 44, 66, 14, 23, 11],\n    E = categorical([missing, 'g', 'r', missing, 'r', 'g', 'p'])\n)\n\nencoder = MissingnessEncoder()\nmach = fit!(machine(encoder, Xm))\nXnew = transform(mach, Xm)\n\njulia> Xnew\n(A = [\"Ben\", \"John\", \"missing\", \"missing\", \"Mary\", \"John\", \"missing\"],\n B = Union{Missing, Float64}[1.85, 1.67, missing, missing, 1.5, 1.67, missing],\n C = [7, 5, -1, -1, 10, 0, -1],\n D = [23, 23, 44, 66, 14, 23, 11],\n E = ['m', 'g', 'r', 'm', 'r', 'g', 'p'],)\n\n\nSee also CardinalityReducer\n\n\n\n\n\n","category":"type"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"CollapsedDocStrings = true","category":"page"},{"location":"contributing/#Adding-new-models-to-MLJTransforms","page":"Contributing","title":"Adding new models to MLJTransforms","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"In this package, data transformers are not implemented using a specific generic template, whereas categorical encoders are due to their systematic nature of encoding categorical levels into scalars or vectors. In light of this, the most pivotal method in implementing a new categorical encoder is:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"MLJTransforms.generic_fit","category":"page"},{"location":"contributing/#MLJTransforms.generic_fit","page":"Contributing","title":"MLJTransforms.generic_fit","text":"generic_fit(X,\n    features = Symbol[],\n    args...;\n    ignore::Bool = true,\n    ordered_factor::Bool = false,\n    feature_mapper,\n    kwargs...,\n)\n\nGiven a feature_mapper (see definition below), this method applies      feature_mapper across a specified subset of categorical columns in X and returns a dictionary      whose keys are the feature names, and each value is the corresponding      level‑to‑value mapping produced by feature_mapper. \n\nIn essence, it spares effort of looping over each column and applying the feature_mapper function manually as well as handling the feature selection logic.\n\nArguments\n\nX: A table where the elements of the categorical features have scitypes    Multiclass or OrderedFactor\nfeatures=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\nfeature_mapper: function that, for a given vector (eg, corresponding to a categorical column from the dataset X),    produces a mapping from each category level name in this vector to a scalar or vector according to specified transformation logic.\n\nNote\n\nAny additional arguments (whether keyword or not) provided to this function are passed to the feature_mapper function which   is helpful when feature_mapper requires additional arguments to compute the mapping (eg, hyperparameters).\n\nReturns\n\nmapping_per_feat_level: Maps each level for each feature in a subset of the categorical features of   X into a scalar or a vector. \nencoded_features: The subset of the categorical features of X that were encoded\n\n\n\n\n\n","category":"function"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"followed by:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"MLJTransforms.generic_transform","category":"page"},{"location":"contributing/#MLJTransforms.generic_transform","page":"Contributing","title":"MLJTransforms.generic_transform","text":"generic_transform(\n    X,\n    mapping_per_feat_level;\n    single_feat::Bool = true,\n    ignore_unknown::Bool = false,\n    use_levelnames::Bool = false,\n    custom_levels = nothing,\n    ensure_categorical::Bool = false,\n)\n\nApply a per‐level feature mapping to selected categorical columns in X, returning a new table of the same type.\n\nArguments\n\nX: A table where the elements of the categorical features have scitypes    Multiclass or OrderedFactor\nmapping_per_feat_level::Dict{Symbol,Dict}:   A dict whose keys are feature names (Symbol) and values are themselves dictionaries    mapping each observed level to either a scalar (if single_feat=true) or a fixed‐length vector        (if single_feat=false). Only columns whose names appear in mapping_per_feat_level are            transformed; others pass through unchanged.\nsingle_feat::Bool=true:   If true, each input level is mapped to a single scalar feature; if false,   each input level is mapped to a length‑k vector, producing k output columns.\nignore_unknown::Bool=false:   If false, novel levels in X (not seen during fit) will raise an error;    if true, novel levels will be left unchanged (identity mapping).\nuse_levelnames::Bool=false:   When single_feat=false, controls naming of the expanded columns: true: use actual level names (e.g. :color_red, :color_blue),    false: use numeric indices (e.g. :color_1, :color_2).\ncustom_levels::Union{Nothing,Vector}:   If not nothing, overrides the names of levels used to generate feature names when single_feat=false.\nensure_categorical::Bool=false:   Only when single_feat=true and if true, preserves the categorical type of the column after        recoding (eg, feature should still be recognized as Multiclass after transformation)\n\nReturns\n\nA new table of potentially similar to X but with categorical columns transformed according to mapping_per_feat_level.\n\n\n\n\n\n","category":"function"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"All categorical encoders in this packager are implemented using these two methods. For an example, see FrequencyEncoder source code.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Moreover, you should implement the MLJModelInterface for any method you provide in this package. Check the interface docs and/or the existing interfaces in this package (eg, this interface for the FrequencyEncoder).","category":"page"},{"location":"transformers/numerical/","page":"-","title":"-","text":"Other Transformers include more generic transformers that go beyond categorical encoding","category":"page"},{"location":"transformers/numerical/","page":"-","title":"-","text":"Transformer Brief Description\nStandardizer Transforming columns of numerical features by standardization\nUnivariateBoxCoxTransformer Apply BoxCox transformation given a single vector\nInteractionTransformer Transforming columns of numerical features to create new interaction features\nUnivariateDiscretizer Discretize a continuous vector into an ordered factor\nFillImputer Fill missing values of features belonging to any finite or infinite scientific type","category":"page"},{"location":"transformers/numerical/","page":"-","title":"-","text":"MLJTransforms.Standardizer","category":"page"},{"location":"transformers/numerical/#MLJTransforms.Standardizer","page":"-","title":"MLJTransforms.Standardizer","text":"Standardizer\n\nA model type for constructing a standardizer, based on unknown.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nStandardizer = @load Standardizer pkg=unknown\n\nDo model = Standardizer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in Standardizer(features=...).\n\nUse this model to standardize (whiten) a Continuous vector, or relevant columns of a table. The rescalings applied by this transformer to new data are always those learned during the training phase, which are generally different from what would actually standardize the new data.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any Tables.jl compatible table or any abstract vector with Continuous element scitype (any abstract float vector). Only features in a table with Continuous scitype can be standardized; check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures: one of the following, with the behavior indicated below:\n[] (empty, the default): standardize all features (columns) having Continuous element scitype\nnon-empty vector of feature names (symbols): standardize only the Continuous features in the vector (if ignore=false) or Continuous features not named in the vector (ignore=true).\nfunction or other callable: standardize a feature if the callable returns true on its name. For example, Standardizer(features = name -> name in [:x1, :x3], ignore = true, count=true) has the same effect as Standardizer(features = [:x1, :x3], ignore = true, count=true), namely to standardize all Continuous and Count features, with the exception of :x1 and :x3.\nNote this behavior is further modified if the ordered_factor or count flags are set to true; see below\nignore=false: whether to ignore or standardize specified features, as explained above\nordered_factor=false: if true, standardize any OrderedFactor feature wherever a Continuous feature would be standardized, as described above\ncount=false: if true, standardize any Count feature wherever a Continuous feature would be standardized, as described above\n\nOperations\n\ntransform(mach, Xnew): return Xnew with relevant features standardized according to the rescalings learned during fitting of mach.\ninverse_transform(mach, Z): apply the inverse transformation to Z, so that inverse_transform(mach, transform(mach, Xnew)) is approximately the same as Xnew; unavailable if ordered_factor or count flags were set to true.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nfeatures_fit - the names of features that will be standardized\nmeans - the corresponding untransformed mean values\nstds - the corresponding untransformed standard deviations\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures_fit: the names of features that will be standardized\n\nExamples\n\nusing MLJ\n\nX = (ordinal1 = [1, 2, 3],\n     ordinal2 = coerce([:x, :y, :x], OrderedFactor),\n     ordinal3 = [10.0, 20.0, 30.0],\n     ordinal4 = [-20.0, -30.0, -40.0],\n     nominal = coerce([\"Your father\", \"he\", \"is\"], Multiclass));\n\njulia> schema(X)\n┌──────────┬──────────────────┐\n│ names    │ scitypes         │\n├──────────┼──────────────────┤\n│ ordinal1 │ Count            │\n│ ordinal2 │ OrderedFactor{2} │\n│ ordinal3 │ Continuous       │\n│ ordinal4 │ Continuous       │\n│ nominal  │ Multiclass{3}    │\n└──────────┴──────────────────┘\n\nstand1 = Standardizer();\n\njulia> transform(fit!(machine(stand1, X)), X)\n(ordinal1 = [1, 2, 3],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal3 = [-1.0, 0.0, 1.0],\n ordinal4 = [1.0, 0.0, -1.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\nstand2 = Standardizer(features=[:ordinal3, ], ignore=true, count=true);\n\njulia> transform(fit!(machine(stand2, X)), X)\n(ordinal1 = [-1.0, 0.0, 1.0],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal3 = [10.0, 20.0, 30.0],\n ordinal4 = [1.0, 0.0, -1.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\nSee also OneHotEncoder, ContinuousEncoder.\n\n\n\n\n\n","category":"type"},{"location":"transformers/numerical/","page":"-","title":"-","text":"MLJTransforms.InteractionTransformer","category":"page"},{"location":"transformers/numerical/#MLJTransforms.InteractionTransformer","page":"-","title":"MLJTransforms.InteractionTransformer","text":"InteractionTransformer\n\nA model type for constructing a interaction transformer, based on unknown.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nInteractionTransformer = @load InteractionTransformer pkg=unknown\n\nDo model = InteractionTransformer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in InteractionTransformer(order=...).\n\nGenerates all polynomial interaction terms up to the given order for the subset of chosen columns.  Any column that contains elements with scitype <:Infinite is a valid basis to generate interactions.  If features is not specified, all such columns with scitype <:Infinite in the table are used as a basis.\n\nIn MLJ or MLJBase, you can transform features X with the single call\n\ntransform(machine(model), X)\n\nSee also the example below.\n\nHyper-parameters\n\norder: Maximum order of interactions to be generated.\nfeatures: Restricts interations generation to those columns\n\nOperations\n\ntransform(machine(model), X): Generates polynomial interaction terms out of table X using the hyper-parameters specified in model.\n\nExample\n\nusing MLJ\n\nX = (\n    A = [1, 2, 3],\n    B = [4, 5, 6],\n    C = [7, 8, 9],\n    D = [\"x₁\", \"x₂\", \"x₃\"]\n)\nit = InteractionTransformer(order=3)\nmach = machine(it)\n\njulia> transform(mach, X)\n(A = [1, 2, 3],\n B = [4, 5, 6],\n C = [7, 8, 9],\n D = [\"x₁\", \"x₂\", \"x₃\"],\n A_B = [4, 10, 18],\n A_C = [7, 16, 27],\n B_C = [28, 40, 54],\n A_B_C = [28, 80, 162],)\n\nit = InteractionTransformer(order=2, features=[:A, :B])\nmach = machine(it)\n\njulia> transform(mach, X)\n(A = [1, 2, 3],\n B = [4, 5, 6],\n C = [7, 8, 9],\n D = [\"x₁\", \"x₂\", \"x₃\"],\n A_B = [4, 10, 18],)\n\n\n\n\n\n\n","category":"type"},{"location":"transformers/numerical/","page":"-","title":"-","text":"MLJTransforms.BoxCoxTransformer","category":"page"},{"location":"transformers/numerical/","page":"-","title":"-","text":"MLJTransforms.UnivariateDiscretizer","category":"page"},{"location":"transformers/numerical/#MLJTransforms.UnivariateDiscretizer","page":"-","title":"MLJTransforms.UnivariateDiscretizer","text":"UnivariateDiscretizer\n\nA model type for constructing a single variable discretizer, based on unknown.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nUnivariateDiscretizer = @load UnivariateDiscretizer pkg=unknown\n\nDo model = UnivariateDiscretizer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in UnivariateDiscretizer(n_classes=...).\n\nDiscretization converts a Continuous vector into an OrderedFactor vector. In particular, the output is a CategoricalVector (whose reference type is optimized).\n\nThe transformation is chosen so that the vector on which the transformer is fit has, in transformed form, an approximately uniform distribution of values. Specifically, if n_classes is the level of discretization, then 2*n_classes - 1 ordered quantiles are computed, the odd quantiles being used for transforming (discretization) and the even quantiles for inverse transforming.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, x)\n\nwhere\n\nx: any abstract vector with Continuous element scitype; check scitype with scitype(x).\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nn_classes: number of discrete classes in the output\n\nOperations\n\ntransform(mach, xnew): discretize xnew according to the discretization learned when fitting mach\ninverse_transform(mach, z): attempt to reconstruct from z a vector that transforms to give z\n\nFitted parameters\n\nThe fields of fitted_params(mach).fitesult include:\n\nodd_quantiles: quantiles used for transforming (length is n_classes - 1)\neven_quantiles: quantiles used for inverse transforming (length is n_classes)\n\nExample\n\nusing MLJ\nusing Random\nRandom.seed!(123)\n\ndiscretizer = UnivariateDiscretizer(n_classes=100)\nmach = machine(discretizer, randn(1000))\nfit!(mach)\n\njulia> x = rand(5)\n5-element Vector{Float64}:\n 0.8585244609846809\n 0.37541692370451396\n 0.6767070590395461\n 0.9208844241267105\n 0.7064611415680901\n\njulia> z = transform(mach, x)\n5-element CategoricalArrays.CategoricalArray{UInt8,1,UInt8}:\n 0x52\n 0x42\n 0x4d\n 0x54\n 0x4e\n\nx_approx = inverse_transform(mach, z)\njulia> x - x_approx\n5-element Vector{Float64}:\n 0.008224506144777322\n 0.012731354778359405\n 0.0056265330571125816\n 0.005738175684445124\n 0.006835652575801987\n\n\n\n\n\n","category":"type"},{"location":"transformers/numerical/","page":"-","title":"-","text":"MLJTransforms.FillImputer","category":"page"},{"location":"transformers/numerical/#MLJTransforms.FillImputer","page":"-","title":"MLJTransforms.FillImputer","text":"FillImputer\n\nA model type for constructing a fill imputer, based on unknown.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nFillImputer = @load FillImputer pkg=unknown\n\nDo model = FillImputer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in FillImputer(features=...).\n\nUse this model to impute missing values in tabular data. A fixed \"filler\" value is learned from the training data, one for each column of the table.\n\nFor imputing missing values in a vector, use UnivariateFillImputer instead.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose features each have element scitypes Union{Missing, T}, where T is a subtype of Continuous, Multiclass, OrderedFactor or Count. Check scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures: a vector of names of features (symbols) for which imputation is to be attempted; default is empty, which is interpreted as \"impute all\".\ncontinuous_fill: function or other callable to determine value to be imputed in the case of Continuous (abstract float) data; default is to apply median after skipping missing values\ncount_fill: function or other callable to determine value to be imputed in the case of Count (integer) data; default is to apply rounded median after skipping missing values\nfinite_fill: function or other callable to determine value to be imputed in the case of Multiclass or OrderedFactor data (categorical vectors); default is to apply mode after skipping missing values\n\nOperations\n\ntransform(mach, Xnew): return Xnew with missing values imputed with the fill values learned when fitting mach\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nfeatures_seen_in_fit: the names of features (features) encountered during training\nunivariate_transformer: the univariate model applied to determine   the fillers (it's fields contain the functions defining the filler computations)\nfiller_given_feature: dictionary of filler values, keyed on feature (column) names\n\nExamples\n\nusing MLJ\nimputer = FillImputer()\n\nX = (a = [1.0, 2.0, missing, 3.0, missing],\n     b = coerce([\"y\", \"n\", \"y\", missing, \"y\"], Multiclass),\n     c = [1, 1, 2, missing, 3])\n\nschema(X)\njulia> schema(X)\n┌───────┬───────────────────────────────┐\n│ names │ scitypes                      │\n├───────┼───────────────────────────────┤\n│ a     │ Union{Missing, Continuous}    │\n│ b     │ Union{Missing, Multiclass{2}} │\n│ c     │ Union{Missing, Count}         │\n└───────┴───────────────────────────────┘\n\nmach = machine(imputer, X)\nfit!(mach)\n\njulia> fitted_params(mach).filler_given_feature\n(filler = 2.0,)\n\njulia> fitted_params(mach).filler_given_feature\nDict{Symbol, Any} with 3 entries:\n  :a => 2.0\n  :b => \"y\"\n  :c => 2\n\njulia> transform(mach, X)\n(a = [1.0, 2.0, 2.0, 3.0, 2.0],\n b = CategoricalValue{String, UInt32}[\"y\", \"n\", \"y\", \"y\", \"y\"],\n c = [1, 1, 2, 2, 3],)\n\nSee also UnivariateFillImputer.\n\n\n\n\n\n","category":"type"},{"location":"about/#Credits","page":"About","title":"Credits","text":"","category":"section"},{"location":"about/","page":"About","title":"About","text":"This package was created by Essam Wisam as a Google Summer of Code project on categorical encoding, under the mentorship of Anthony Blaom offering over eleven different categorical encoding methods. Subsequently, the package was expanded to include eight other data transformation methods beyond categorical encoders that were originally developed in MLJModels.jl, originally authored by Anthony Blaom, Thibaut Lienart , Samuel Okon, Yiannis Simillides, Dilum Aluthge, Sebastian Vollmer, Darren Christopher Lukas, Mateusz Baran, Shuhei Kadowaki, Olivier Labayle, and others.","category":"page"},{"location":"transformers/classical/","page":"Classical Encoders","title":"Classical Encoders","text":"Classical encoders include well known and commonly used categorical encoders:","category":"page"},{"location":"transformers/classical/","page":"Classical Encoders","title":"Classical Encoders","text":"Transformer Brief Description\nOneHotEncoder Encode categorical variables into one-hot vectors\nContinuousEncoder Adds type casting functionality to OnehotEncoder\nOrdinalEncoder Encode categorical variables into ordered integers\nFrequencyEncoder Encode categorical variables into their normalized or unormalized frequencies\nTargetEncoder Encode categorical variables into relevant target statistics","category":"page"},{"location":"transformers/classical/","page":"Classical Encoders","title":"Classical Encoders","text":"MLJTransforms.OneHotEncoder","category":"page"},{"location":"transformers/classical/#MLJTransforms.OneHotEncoder","page":"Classical Encoders","title":"MLJTransforms.OneHotEncoder","text":"OneHotEncoder\n\nA model type for constructing a one-hot encoder, based on unknown.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nOneHotEncoder = @load OneHotEncoder pkg=unknown\n\nDo model = OneHotEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in OneHotEncoder(features=...).\n\nUse this model to one-hot encode the Multiclass and OrderedFactor features (columns) of some table, leaving other columns unchanged.\n\nNew data to be transformed may lack features present in the fit data, but no new features can be present.\n\nWarning: This transformer assumes that levels(col) for any Multiclass or OrderedFactor column, col, is the same for training data and new data to be transformed.\n\nTo ensure all features are transformed into Continuous features, or dropped, use ContinuousEncoder instead.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any Tables.jl compatible table. Columns can be of mixed type but only those with element scitype Multiclass or OrderedFactor can be encoded. Check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures: a vector of symbols (feature names). If empty (default) then all Multiclass and OrderedFactor features are encoded. Otherwise, encoding is further restricted to the specified features (ignore=false) or the unspecified features (ignore=true). This default behavior can be modified by the ordered_factor flag.\nordered_factor=false: when true, OrderedFactor features are universally excluded\ndrop_last=true: whether to drop the column corresponding to the final class of encoded features. For example, a three-class feature is spawned into three new features if drop_last=false, but just two features otherwise.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nall_features: names of all features encountered in training\nfitted_levels_given_feature: dictionary of the levels associated with each feature encoded, keyed on the feature name\nref_name_pairs_given_feature: dictionary of pairs r => ftr (such as 0x00000001 => :grad__A) where r is a CategoricalArrays.jl reference integer representing a level, and ftr the corresponding new feature name; the dictionary is keyed on the names of features that are encoded\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures_to_be_encoded: names of input features to be encoded\nnew_features: names of all output features\n\nExample\n\nusing MLJ\n\nX = (name=categorical([\"Danesh\", \"Lee\", \"Mary\", \"John\"]),\n     grade=categorical([\"A\", \"B\", \"A\", \"C\"], ordered=true),\n     height=[1.85, 1.67, 1.5, 1.67],\n     n_devices=[3, 2, 4, 3])\n\njulia> schema(X)\n┌───────────┬──────────────────┐\n│ names     │ scitypes         │\n├───────────┼──────────────────┤\n│ name      │ Multiclass{4}    │\n│ grade     │ OrderedFactor{3} │\n│ height    │ Continuous       │\n│ n_devices │ Count            │\n└───────────┴──────────────────┘\n\nhot = OneHotEncoder(drop_last=true)\nmach = fit!(machine(hot, X))\nW = transform(mach, X)\n\njulia> schema(W)\n┌──────────────┬────────────┐\n│ names        │ scitypes   │\n├──────────────┼────────────┤\n│ name__Danesh │ Continuous │\n│ name__John   │ Continuous │\n│ name__Lee    │ Continuous │\n│ grade__A     │ Continuous │\n│ grade__B     │ Continuous │\n│ height       │ Continuous │\n│ n_devices    │ Count      │\n└──────────────┴────────────┘\n\nSee also ContinuousEncoder.\n\n\n\n\n\n","category":"type"},{"location":"transformers/classical/","page":"Classical Encoders","title":"Classical Encoders","text":"MLJTransforms.ContinuousEncoder","category":"page"},{"location":"transformers/classical/#MLJTransforms.ContinuousEncoder","page":"Classical Encoders","title":"MLJTransforms.ContinuousEncoder","text":"ContinuousEncoder\n\nA model type for constructing a continuous encoder, based on unknown.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nContinuousEncoder = @load ContinuousEncoder pkg=unknown\n\nDo model = ContinuousEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ContinuousEncoder(drop_last=...).\n\nUse this model to arrange all features (features) of a table to have Continuous element scitype, by applying the following protocol to each feature ftr:\n\nIf ftr is already Continuous retain it.\nIf ftr is Multiclass, one-hot encode it.\nIf ftr is OrderedFactor, replace it with coerce(ftr, Continuous) (vector of floating point integers), unless ordered_factors=false is specified, in which case one-hot encode it.\nIf ftr is Count, replace it with coerce(ftr, Continuous).\nIf ftr has some other element scitype, or was not observed in fitting the encoder, drop it from the table.\n\nWarning: This transformer assumes that levels(col) for any Multiclass or OrderedFactor column, col, is the same for training data and new data to be transformed.\n\nTo selectively one-hot-encode categorical features (without dropping features) use OneHotEncoder instead.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any Tables.jl compatible table. features can be of mixed type but only those with element scitype Multiclass or OrderedFactor can be encoded. Check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\ndrop_last=true: whether to drop the column corresponding to the final class of one-hot encoded features. For example, a three-class feature is spawned into three new features if drop_last=false, but two just features otherwise.\none_hot_ordered_factors=false: whether to one-hot any feature with OrderedFactor element scitype, or to instead coerce it directly to a (single) Continuous feature using the order\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nfeatures_to_keep: names of features that will not be dropped from the table\none_hot_encoder: the OneHotEncoder model instance for handling the one-hot encoding\none_hot_encoder_fitresult: the fitted parameters of the OneHotEncoder model\n\nReport\n\nfeatures_to_keep: names of input features that will not be dropped from the table\nnew_features: names of all output features\n\nExample\n\nX = (name=categorical([\"Danesh\", \"Lee\", \"Mary\", \"John\"]),\n     grade=categorical([\"A\", \"B\", \"A\", \"C\"], ordered=true),\n     height=[1.85, 1.67, 1.5, 1.67],\n     n_devices=[3, 2, 4, 3],\n     comments=[\"the force\", \"be\", \"with you\", \"too\"])\n\njulia> schema(X)\n┌───────────┬──────────────────┐\n│ names     │ scitypes         │\n├───────────┼──────────────────┤\n│ name      │ Multiclass{4}    │\n│ grade     │ OrderedFactor{3} │\n│ height    │ Continuous       │\n│ n_devices │ Count            │\n│ comments  │ Textual          │\n└───────────┴──────────────────┘\n\nencoder = ContinuousEncoder(drop_last=true)\nmach = fit!(machine(encoder, X))\nW = transform(mach, X)\n\njulia> schema(W)\n┌──────────────┬────────────┐\n│ names        │ scitypes   │\n├──────────────┼────────────┤\n│ name__Danesh │ Continuous │\n│ name__John   │ Continuous │\n│ name__Lee    │ Continuous │\n│ grade        │ Continuous │\n│ height       │ Continuous │\n│ n_devices    │ Continuous │\n└──────────────┴────────────┘\n\njulia> setdiff(schema(X).names, report(mach).features_to_keep) # dropped features\n1-element Vector{Symbol}:\n :comments\n\n\nSee also OneHotEncoder\n\n\n\n\n\n","category":"type"},{"location":"transformers/classical/","page":"Classical Encoders","title":"Classical Encoders","text":"MLJTransforms.OrdinalEncoder","category":"page"},{"location":"transformers/classical/#MLJTransforms.OrdinalEncoder","page":"Classical Encoders","title":"MLJTransforms.OrdinalEncoder","text":"OrdinalEncoder\n\nA model type for constructing a ordinal encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nOrdinalEncoder = @load OrdinalEncoder pkg=MLJTransforms\n\nDo model = OrdinalEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in OrdinalEncoder(features=...).\n\nOrdinalEncoder implements ordinal encoding which replaces the categorical values in the specified     categorical features with integers (ordered arbitrarily). This will create an implicit ordering between     categories which may not be a proper modelling assumption.\n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to   check scitypes. \n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\noutput_type: The numerical concrete type of the encoded features. Default is Float32.\n\nOperations\n\ntransform(mach, Xnew): Apply ordinal encoding to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and   return the new table.   Features that are neither Multiclass nor OrderedFactor  are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nindex_given_feat_level: A dictionary that maps each level for each column in a subset of the categorical features of X into an integer. \n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nusing MLJ\n\n# Define categorical features\nA = [\"g\", \"b\", \"g\", \"r\", \"r\",]  \nB = [1.0, 2.0, 3.0, 4.0, 5.0,]\nC = [\"f\", \"f\", \"f\", \"m\", \"f\",]  \nD = [true, false, true, false, true,]\nE = [1, 2, 3, 4, 5,]\n\n# Combine into a named tuple\nX = (A = A, B = B, C = C, D = D, E = E)\n\n# Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Continuous,\n:C => Multiclass,\n:D => Multiclass,\n:E => OrderedFactor,\n)\n\n# Check scitype coercion:\nschema(X)\n\nencoder = OrdinalEncoder(ordered_factor = false)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia > Xnew\n    (A = [2, 1, 2, 3, 3],\n    B = [1.0, 2.0, 3.0, 4.0, 5.0],\n    C = [1, 1, 1, 2, 1],\n    D = [2, 1, 2, 1, 2],\n    E = CategoricalArrays.CategoricalValue{Int64, UInt32}[1, 2, 3, 4, 5],)\n\nSee also TargetEncoder\n\n\n\n\n\n","category":"type"},{"location":"transformers/classical/","page":"Classical Encoders","title":"Classical Encoders","text":"MLJTransforms.FrequencyEncoder","category":"page"},{"location":"transformers/classical/#MLJTransforms.FrequencyEncoder","page":"Classical Encoders","title":"MLJTransforms.FrequencyEncoder","text":"FrequencyEncoder\n\nA model type for constructing a frequency encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nFrequencyEncoder = @load FrequencyEncoder pkg=MLJTransforms\n\nDo model = FrequencyEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in FrequencyEncoder(features=...).\n\nFrequencyEncoder implements frequency encoding which replaces the categorical values in the specified     categorical features with their (normalized or raw) frequencies of occurrence in the dataset. \n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to   check scitypes. \n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\nnormalize=false: Whether to use normalized frequencies that sum to 1 over category values or to use raw counts.\noutput_type=Float32: The type of the output values. The default is Float32, but you can set it to Float64 or any other type that can hold the frequency values.\n\nOperations\n\ntransform(mach, Xnew): Apply frequency encoding to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and   return the new table.   Features that are neither Multiclass nor OrderedFactor  are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nstatistic_given_feat_val: A dictionary that maps each level for each column in a subset of the categorical features of X into its frequency.\n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nusing MLJ\n\n# Define categorical features\nA = [\"g\", \"b\", \"g\", \"r\", \"r\",]  \nB = [1.0, 2.0, 3.0, 4.0, 5.0,]\nC = [\"f\", \"f\", \"f\", \"m\", \"f\",]  \nD = [true, false, true, false, true,]\nE = [1, 2, 3, 4, 5,]\n\n# Combine into a named tuple\nX = (A = A, B = B, C = C, D = D, E = E)\n\n# Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Continuous,\n:C => Multiclass,\n:D => Multiclass,\n:E => OrderedFactor,\n)\n\n# Check scitype coercions:\nschema(X)\n\nencoder = FrequencyEncoder(ordered_factor = false, normalize=true)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia > Xnew\n    (A = [2, 1, 2, 2, 2],\n    B = [1.0, 2.0, 3.0, 4.0, 5.0],\n    C = [4, 4, 4, 1, 4],\n    D = [3, 2, 3, 2, 3],\n    E = CategoricalArrays.CategoricalValue{Int64, UInt32}[1, 2, 3, 4, 5],)\n\nSee also TargetEncoder\n\n\n\n\n\n","category":"type"},{"location":"transformers/classical/","page":"Classical Encoders","title":"Classical Encoders","text":"MLJTransforms.TargetEncoder","category":"page"},{"location":"transformers/classical/#MLJTransforms.TargetEncoder","page":"Classical Encoders","title":"MLJTransforms.TargetEncoder","text":"TargetEncoder\n\nA model type for constructing a target encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nTargetEncoder = @load TargetEncoder pkg=MLJTransforms\n\nDo model = TargetEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in TargetEncoder(features=...).\n\nTargetEncoder implements target encoding as defined in [1] to encode categorical variables      into continuous ones using statistics from the target variable.\n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to   check scitypes. \n\ny is the target, which can be any AbstractVector whose element scitype is Continuous or Count for regression problems and  Multiclass or OrderedFactor for classification problems; check the scitype with schema(y)\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\nλ: Shrinkage hyperparameter used to mix between posterior and prior statistics as described in [1]\nm: An integer hyperparameter to compute shrinkage as described in [1]. If m=:auto then m will be computed using\n\nempirical Bayes estimation as described in [1]\n\nOperations\n\ntransform(mach, Xnew): Apply target encoding to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and   return the new table.   Features that are neither Multiclass nor OrderedFactor  are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\ntask: Whether the task is Classification or Regression\ny_statistic_given_feat_level: A dictionary with the necessary statistics to encode each categorical feature. It maps each    level in each categorical feature to a statistic computed over the target.\n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nusing MLJ\n\n# Define categorical features\nA = [\"g\", \"b\", \"g\", \"r\", \"r\",]  \nB = [1.0, 2.0, 3.0, 4.0, 5.0,]\nC = [\"f\", \"f\", \"f\", \"m\", \"f\",]  \nD = [true, false, true, false, true,]\nE = [1, 2, 3, 4, 5,]\n\n# Define the target variable \ny = [\"c1\", \"c2\", \"c3\", \"c1\", \"c2\",]\n\n# Combine into a named tuple\nX = (A = A, B = B, C = C, D = D, E = E)\n\n# Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Continuous,\n:C => Multiclass,\n:D => Multiclass,\n:E => OrderedFactor,\n)\ny = coerce(y, Multiclass)\n\nencoder = TargetEncoder(ordered_factor = false, lambda = 1.0, m = 0,)\nmach = fit!(machine(encoder, X, y))\nXnew = transform(mach, X)\n\njulia > schema(Xnew)\n┌───────┬──────────────────┬─────────────────────────────────┐\n│ names │ scitypes         │ types                           │\n├───────┼──────────────────┼─────────────────────────────────┤\n│ A_1   │ Continuous       │ Float64                         │\n│ A_2   │ Continuous       │ Float64                         │\n│ A_3   │ Continuous       │ Float64                         │\n│ B     │ Continuous       │ Float64                         │\n│ C_1   │ Continuous       │ Float64                         │\n│ C_2   │ Continuous       │ Float64                         │\n│ C_3   │ Continuous       │ Float64                         │\n│ D_1   │ Continuous       │ Float64                         │\n│ D_2   │ Continuous       │ Float64                         │\n│ D_3   │ Continuous       │ Float64                         │\n│ E     │ OrderedFactor{5} │ CategoricalValue{Int64, UInt32} │\n└───────┴──────────────────┴─────────────────────────────────┘\n\nReference\n\n[1] Micci-Barreca, Daniele.      “A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems”      SIGKDD Explor. Newsl. 3, 1 (July 2001), 27–32.\n\nSee also OneHotEncoder\n\n\n\n\n\n","category":"type"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"EditURL = \"notebook.jl\"","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Entity-Embeddings-Tutorial","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"Julia version is assumed to be 1.10.*","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"This demonstration is available as a Jupyter notebook or julia script (as well as the dataset) here.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"Entity embedding is a newer deep learning approach for categorical encoding introduced in 2016 by Cheng Guo and Felix Berkhahn. It employs a set of embedding layers to map each categorical feature into a dense continuous vector in a similar fashion to how they are employed in NLP architectures.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"In MLJFlux, the EntityEmbedder provides a high-level interface to learn entity embeddings using any supervised MLJFlux model as the underlying learner. The embedder can be used as a transformer in MLJ pipelines to encode categorical features with learned embeddings, which can then be used as features in downstream machine learning models.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"In this tutorial, we will explore how to use the EntityEmbedder to learn and apply entity embeddings on the Google Play Store dataset.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Learning-Objectives","page":"Entity Embeddings Tutorial","title":"Learning Objectives","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"Understand the concept of entity embeddings for categorical encoding\nLearn how to use EntityEmbedder from MLJFlux\nApply entity embeddings to a real-world dataset\nVisualize the learned embedding spaces\nBuild pipelines combining embeddings with downstream models","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"using Pkg;\nPkg.activate(@__DIR__);\n\n\n\n# Import all required packages\nusing MLJ\nusing CategoricalArrays\nusing DataFrames\nusing Optimisers\nusing Random\nusing Tables\nusing ProgressMeter\nusing Plots\nusing ScientificTypes\nusing CSV\nusing StatsBase  ## For countmap\nimport Plots: mm  ## For margin units","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"  Activating project at `~/Documents/GitHub/MLJTransforms/docs/src/tutorials/entity_embeddings`\n","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Data-Loading-and-Preprocessing","page":"Entity Embeddings Tutorial","title":"Data Loading and Preprocessing","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"We'll use the Google Play Store dataset which contains information about mobile applications. This dataset has several categorical features that are perfect for demonstrating entity embeddings:","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"Category: App category (e.g., Games, Social, Tools)\nContent Rating: Age rating (e.g., Everyone, Teen, Mature)\nGenres: Primary genre of the app\nAndroid Ver: Required Android version\nType: Free or Paid","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Load the Google Play Store dataset\ndf = CSV.read(\"./googleplaystore.csv\", DataFrame);","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"┌ Warning: thread = 1 warning: only found 12 / 13 columns around data row: 10473. Filling remaining columns with `missing`\n└ @ CSV ~/.julia/packages/CSV/XLcqT/src/file.jl:592\n┌ Warning: thread = 1 warning: only found 12 / 13 columns around data row: 10473. Filling remaining columns with `missing`\n└ @ CSV ~/.julia/packages/CSV/XLcqT/src/file.jl:592\n","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Data-Cleaning-and-Type-Conversion","page":"Entity Embeddings Tutorial","title":"Data Cleaning and Type Conversion","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"The raw dataset requires significant cleaning. We'll handle:","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"Reviews: Convert to integers\nSize: Parse size strings like \"14M\", \"512k\" to numeric values\nInstalls: Remove formatting characters and convert to integers\nPrice: Remove dollar signs and convert to numeric\nGenres: Extract primary genre only","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Custom parsing function that returns missing instead of nothing\nsafe_parse(T, s) = something(tryparse(T, s), missing);\n\n# Reviews: ensure integer\ndf.Reviews = safe_parse.(Int, string.(df.Reviews));\n\n# Size: \"14M\", \"512k\", or \"Varies with device\"\nfunction parse_size(s)\n    if s == \"Varies with device\"\n        return missing\n    elseif occursin('M', s)\n        return safe_parse(Float64, replace(s, \"M\" => \"\")) * 1_000_000\n    elseif occursin('k', s)\n        return safe_parse(Float64, replace(s, \"k\" => \"\")) * 1_000\n    else\n        return safe_parse(Float64, s)\n    end\nend\ndf.Size = parse_size.(string.(df.Size));\n\n# Installs: strip '+' and ',' then parse\nclean_installs = replace.(string.(df.Installs), r\"[+,]\" => \"\")\ndf.Installs = safe_parse.(Int, clean_installs);\n\n# Price: strip leading '$'\ndf.Price = safe_parse.(Float64, replace.(string.(df.Price), r\"^\\$\" => \"\"));\n\n# Genres: take only the primary genre\ndf.Genres = first.(split.(string.(df.Genres), ';'));","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Storing-Category-Information-for-Visualization","page":"Entity Embeddings Tutorial","title":"Storing Category Information for Visualization","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"We'll store the unique values of each categorical feature to use later when visualizing the embeddings.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Store unique category names for visualization later\ncategory_names = Dict(\n    :Category => sort(unique(df.Category)),\n    Symbol(\"Content Rating\") => sort(unique(df[!, Symbol(\"Content Rating\")])),\n    :Genres => sort(unique(df.Genres)),\n    Symbol(\"Android Ver\") => sort(unique(df[!, Symbol(\"Android Ver\")])),\n);\n\nprintln(\"Category names extracted:\")\nfor (feature, names) in category_names\n    println(\"$feature: $(length(names)) categories\")\nend","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"Category names extracted:\nCategory: 34 categories\nContent Rating: 7 categories\nAndroid Ver: 35 categories\nGenres: 49 categories\n","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Feature-Selection-and-Missing-Value-Handling","page":"Entity Embeddings Tutorial","title":"Feature Selection and Missing Value Handling","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"We'll select the most relevant features and remove any rows with missing values to ensure clean data for our embedding model.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"select!(\n    df,\n    [\n        :Category, :Reviews, :Size, :Installs, :Type,\n        :Price, Symbol(\"Content Rating\"), :Genres, Symbol(\"Android Ver\"), :Rating,\n    ],\n);\ndropmissing!(df);","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Creating-Categorical-Target-Variable","page":"Entity Embeddings Tutorial","title":"Creating Categorical Target Variable","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"For this tutorial, we'll convert the continuous rating into a categorical classification problem. This will allow us to use a classification model that can learn meaningful embeddings.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"We'll create 10 rating categories by rounding to the nearest 0.5 (e.g., 0.0, 0.5, 1.0, ..., 4.5, 5.0).","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Create 10 classes: 0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5\nfunction rating_to_categorical(rating)\n    # Clamp rating to valid range and round to nearest 0.5\n    clamped_rating = clamp(rating, 0.0, 5.0)\n    rounded_rating = round(clamped_rating * 2) / 2  ## Round to nearest 0.5\n    return string(rounded_rating)\nend\n\n# Apply the transformation\ndf.RatingCategory = categorical([rating_to_categorical(r) for r in df.Rating]);\n\n# Check the distribution of categorical rating labels\nprintln(\"Distribution of categorical rating labels:\")\nprintln(sort(countmap(df.RatingCategory)))\nprintln(\"\\nUnique rating categories: $(sort(unique(df.RatingCategory)))\")","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"Distribution of categorical rating labels:\nOrderedCollections.OrderedDict{CategoricalArrays.CategoricalValue{String, UInt32}, Int64}(\"1.0\" => 17, \"1.5\" => 18, \"2.0\" => 53, \"2.5\" => 105, \"3.0\" => 281, \"3.5\" => 722, \"4.0\" => 2420, \"4.5\" => 3542, \"5.0\" => 571, \"NaN\" => 1416)\n\nUnique rating categories: [\"1.0\", \"1.5\", \"2.0\", \"2.5\", \"3.0\", \"3.5\", \"4.0\", \"4.5\", \"5.0\", \"NaN\"]\n","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Type-Coercion-for-MLJ","page":"Entity Embeddings Tutorial","title":"Type Coercion for MLJ","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"MLJ requires explicit type coercion to understand which columns are categorical vs continuous. This step is crucial for the EntityEmbedder to identify which features need embedding layers.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Coerce types for MLJ compatibility\ndf = coerce(df,\n    :Category => Multiclass,\n    :Reviews => Continuous,\n    :Size => Continuous,\n    :Installs => Continuous,\n    :Type => Multiclass,\n    :Price => Continuous,\n    Symbol(\"Content Rating\") => Multiclass,\n    :Genres => Multiclass,\n    Symbol(\"Android Ver\") => Multiclass,\n    :Rating => Continuous,  ## Keep original for reference\n    :RatingCategory => Multiclass,  ## New categorical target\n);\nschema(df)","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"┌────────────────┬────────────────┬────────────────────────────────────┐\n│ names          │ scitypes       │ types                              │\n├────────────────┼────────────────┼────────────────────────────────────┤\n│ Category       │ Multiclass{33} │ CategoricalValue{String31, UInt32} │\n│ Reviews        │ Continuous     │ Float64                            │\n│ Size           │ Continuous     │ Float64                            │\n│ Installs       │ Continuous     │ Float64                            │\n│ Type           │ Multiclass{2}  │ CategoricalValue{String7, UInt32}  │\n│ Price          │ Continuous     │ Float64                            │\n│ Content Rating │ Multiclass{6}  │ CategoricalValue{String15, UInt32} │\n│ Genres         │ Multiclass{48} │ CategoricalValue{String, UInt32}   │\n│ Android Ver    │ Multiclass{34} │ CategoricalValue{String31, UInt32} │\n│ Rating         │ Continuous     │ Float64                            │\n│ RatingCategory │ Multiclass{10} │ CategoricalValue{String, UInt32}   │\n└────────────────┴────────────────┴────────────────────────────────────┘\n","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Data-Splitting","page":"Entity Embeddings Tutorial","title":"Data Splitting","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"We'll split our data into training and testing sets using stratified sampling to ensure balanced representation of rating categories.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Split into features and target\ny = df[!, :RatingCategory]  ## Use categorical rating as target\nX = select(df, Not([:Rating, :RatingCategory]));  ## Exclude both rating columns from features\n\n# Split the data with stratification\n(X_train, X_test), (y_train, y_test) = partition(\n    (X, y),\n    0.8,\n    multi = true,\n    shuffle = true,\n    stratify = y,\n    rng = Random.Xoshiro(41),\n);\n\nusing MLJFlux","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Building-the-EntityEmbedder-Model","page":"Entity Embeddings Tutorial","title":"Building the EntityEmbedder Model","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Load the neural network classifier\nNeuralNetworkClassifier = @load NeuralNetworkClassifier pkg = MLJFlux","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"MLJFlux.NeuralNetworkClassifier","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Configuring-the-Base-Neural-Network","page":"Entity Embeddings Tutorial","title":"Configuring the Base Neural Network","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"We'll create a neural network classifier with custom embedding dimensions for each categorical feature. Setting smaller embedding dimensions (like 2D) makes it easier to visualize the learned representations.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Create the underlying supervised model that will learn the embeddings\nbase_clf = NeuralNetworkClassifier(\n    builder = MLJFlux.Short(n_hidden = 14),\n    optimiser = Optimisers.Adam(10e-2),\n    batch_size = 20,\n    epochs = 5,\n    acceleration = CUDALibs(),\n    embedding_dims = Dict(\n        :Category => 2,\n        :Type => 2,\n        Symbol(\"Content Rating\") => 2,\n        :Genres => 2,\n        Symbol(\"Android Ver\") => 2,\n    ),\n    rng = 39,\n);","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"┌ Info: The CUDA functionality is being called but\n│ `CUDA.jl` must be loaded to access it.\n└ Add `using CUDA` or `import CUDA` to your code.  Alternatively, configure a different GPU backend by calling `Flux.gpu_backend!`.\n┌ Warning: `acceleration isa CUDALibs` but no CUDA device (GPU) currently live. Specifying an RNG seed when `acceleration isa CUDALibs()` may fail for layers depending on an RNG during training, such as `Dropout`. Consider using  `Random.default_rng()` instead. `\n└ @ MLJFlux ~/.julia/packages/MLJFlux/5eWpt/src/types.jl:62\n","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Creating-the-EntityEmbedder","page":"Entity Embeddings Tutorial","title":"Creating the EntityEmbedder","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"The EntityEmbedder wraps our neural network and can be used as a transformer in MLJ pipelines. By default, it uses min(n_categories - 1, 10) dimensions for any categorical feature not explicitly specified.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Create the EntityEmbedder using the neural network\nembedder = EntityEmbedder(base_clf)","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"EntityEmbedder(\n  model = NeuralNetworkClassifier(\n        builder = Short(n_hidden = 14, …), \n        finaliser = NNlib.softmax, \n        optimiser = Adam(0.1, (0.9, 0.999), 1.0e-8), \n        loss = Flux.Losses.crossentropy, \n        epochs = 5, \n        batch_size = 20, \n        lambda = 0.0, \n        alpha = 0.0, \n        rng = 39, \n        optimiser_changes_trigger_retraining = false, \n        acceleration = ComputationalResources.CUDALibs{Nothing}(nothing), \n        embedding_dims = Dict{Symbol, Real}(:Category => 2, Symbol(\"Content Rating\") => 2, Symbol(\"Android Ver\") => 2, :Genres => 2, :Type => 2)))","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Training-the-EntityEmbedder","page":"Entity Embeddings Tutorial","title":"Training the EntityEmbedder","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"Now we'll train the embedder on our training data. The model learns to predict app ratings while simultaneously learning meaningful embeddings for categorical features.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Create and train the machine\nmach = machine(embedder, X_train, y_train)\nMLJ.fit!(mach, force = true, verbosity = 1);","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"[ Info: Training machine(EntityEmbedder(model = NeuralNetworkClassifier(builder = Short(n_hidden = 14, …), …)), …).\n┌ Info: The CUDA functionality is being called but\n│ `CUDA.jl` must be loaded to access it.\n└ Add `using CUDA` or `import CUDA` to your code.  Alternatively, configure a different GPU backend by calling `Flux.gpu_backend!`.\n[ Info: MLJFlux: converting input data to Float32\nOptimising neural net:  33%[========>                ]  ETA: 0:00:00\u001b[KOptimising neural net:  50%[============>            ]  ETA: 0:00:00\u001b[KOptimising neural net:  67%[================>        ]  ETA: 0:00:00\u001b[KOptimising neural net:  83%[====================>    ]  ETA: 0:00:00\u001b[KOptimising neural net: 100%[=========================] Time: 0:00:00\u001b[K\n","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Transforming-Data-with-Learned-Embeddings","page":"Entity Embeddings Tutorial","title":"Transforming Data with Learned Embeddings","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"After training, we can use the embedder as a transformer to convert categorical features into their learned embedding representations.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Transform the data using the learned embeddings\nX_train_embedded = MLJFlux.transform(mach, X_train)\nX_test_embedded = MLJFlux.transform(mach, X_test);\n\n# Check the schema transformation\nprintln(\"Original schema:\")\nschema(X_train)\nprintln(\"\\nEmbedded schema:\")\nschema(X_train_embedded)","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"Original schema:\n\nEmbedded schema:\n","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Using-Embeddings-in-ML-Pipelines","page":"Entity Embeddings Tutorial","title":"Using Embeddings in ML Pipelines","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"One of the key advantages of entity embeddings is that they can be used as features in any downstream machine learning model. Let's create a pipeline that combines our EntityEmbedder with a k-nearest neighbors classifier.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Load KNN classifier\nKNNClassifier = @load KNNClassifier pkg = NearestNeighborModels\n\n# Create a pipeline: EntityEmbedder -> KNNClassifier\npipe = embedder |> KNNClassifier(K = 5);\n\n# Train the pipeline\npipe_mach = machine(pipe, X_train, y_train)\nMLJ.fit!(pipe_mach, verbosity = 0)","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"trained Machine; does not cache data\n  model: ProbabilisticPipeline(entity_embedder = EntityEmbedder(model = NeuralNetworkClassifier(builder = Short(n_hidden = 14, …), …)), …)\n  args: \n    1:\tSource @225 ⏎ ScientificTypesBase.Table{Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Multiclass{33}}, AbstractVector{ScientificTypesBase.Multiclass{2}}, AbstractVector{ScientificTypesBase.Multiclass{6}}, AbstractVector{ScientificTypesBase.Multiclass{48}}, AbstractVector{ScientificTypesBase.Multiclass{34}}}}\n    2:\tSource @148 ⏎ AbstractVector{ScientificTypesBase.Multiclass{10}}\n","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Visualizing-the-Learned-Embedding-Spaces","page":"Entity Embeddings Tutorial","title":"Visualizing the Learned Embedding Spaces","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"One of the most powerful aspects of entity embeddings is their interpretability. Since we used 2D embeddings, we can visualize how the model has organized different categories in the embedding space.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Extract the learned embedding matrices from the fitted model\nmapping_matrices = fitted_params(mach)[4]","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"Dict{Symbol, Matrix{Float32}} with 5 entries:\n  :Category => [-0.334237 -0.0392749 0.104473 0.256099 -0.0655005 -0.141202 -0.0970246 0.179792 -0.270771 -0.214171 -0.12692 0.41105 0.256761 -0.0494666 -0.111133 0.285277 -0.331778 0.328676 -0.342993 -0.129262 -0.230373 -0.038213 0.108276 -0.153902 -0.324451 -0.237727 0.0345672 -0.0572035 -0.0585397 -0.288544 -0.242574 0.257894 0.00108838; 0.165236 -0.296432 -0.404019 -0.294493 0.185582 0.309341 -0.264846 -0.0410865 0.262034 0.384784 -0.0927044 0.0317509 0.232903 -0.406631 0.288323 0.0836039 0.334631 0.293926 0.290643 -0.334773 -0.306882 -0.00893126 0.185925 -0.309297 0.237027 0.0541817 0.39381 -0.400486 -0.123453 -0.163497 -0.00332076 0.0662401 0.160035]\n  Symbol(\"Content Rating\") => [0.133581 -0.728943 0.0488496 -0.17392 0.32843 0.563167; 0.525009 -0.258198 0.859755 -0.263517 -0.040722 0.33474]\n  Symbol(\"Android Ver\") => [0.170607 0.141154 -0.39162 0.403547 0.196919 -0.384571 -0.148519 -0.130053 0.170175 0.0752836 0.0634965 0.188923 0.179924 -0.346638 0.213056 0.104962 -0.0845368 -0.154221 -0.121818 -0.303438 0.275882 -0.201961 0.0978208 0.212295 -0.00877398 -0.296715 -0.0364835 -0.285863 -0.249095 0.00626427 -0.262972 0.152306 -0.131261 0.177531; -0.100683 -0.194721 0.364714 -0.199976 0.0840546 0.196454 -0.288194 0.215619 -0.282248 -0.0873126 -0.351141 0.0446604 0.37886 -0.151878 0.173701 -0.285421 -0.288342 0.270182 -0.181768 0.0513307 -0.0748576 0.303714 -0.230258 -0.123035 0.147936 0.169242 0.172273 -0.048809 -0.19322 0.239702 -0.130154 -0.407279 0.158713 0.322129]\n  :Genres => [-0.146774 -0.310947 0.163455 0.186525 0.209192 -0.0957032 -0.309449 0.0804618 -0.289171 -0.105948 0.18964 -0.0232951 -0.303402 0.104239 0.213892 -0.0944015 -0.0128954 0.25166 -0.14625 0.340541 0.278481 0.118645 0.049088 -0.269987 -0.336882 -0.0115204 -0.121047 -0.269735 0.233099 -0.159103 -0.0351166 -0.102524 0.0211413 -0.0635436 -0.309698 -0.150375 -0.294988 0.243549 -0.121398 -0.257364 0.309162 0.260066 -0.32487 0.0512156 -0.113218 -0.128149 -0.285641 0.32328; -0.0240456 -0.0927238 -0.320587 -0.337704 0.265411 -0.102303 0.254456 0.288764 0.0950742 0.343972 -0.0132413 0.106584 0.0865783 -0.0157523 -0.121958 0.205434 -0.0508129 0.200128 -0.330792 0.337839 0.226497 0.131038 0.0368269 -0.243523 -0.259971 -0.144437 0.282105 0.223037 -0.0473506 0.131549 -0.22308 0.0791805 -0.17695 0.0639658 -0.344229 -0.274745 0.238469 0.0954359 0.340802 -0.0461901 -0.242701 0.016714 -0.0969492 0.142659 -0.0661325 0.165366 0.0977414 0.255571]\n  :Type => [-1.02336 1.02435; -0.887775 0.944183]","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Creating-Embedding-Visualization-Function","page":"Entity Embeddings Tutorial","title":"Creating Embedding Visualization Function","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"We'll create a helper function to plot the 2D embedding space for each categorical feature. Each point represents a category, and its position shows how the model learned to represent it.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Function to create and display scatter plot for categorical embeddings\nfunction plot_categorical_embeddings(feature_name, feature_categories, embedding_matrix)\n    # Convert feature_name to string to handle both Symbol and String inputs\n    feature_name_str = string(feature_name)\n\n    # Create scatter plot for this feature's embeddings\n    p = scatter(embedding_matrix[1, :], embedding_matrix[2, :],\n        title = \"$(feature_name_str) Embeddings\",\n        xlabel = \"Dimension 1\",\n        ylabel = \"Dimension 2\",\n        label = \"$(feature_name_str)\",\n        legend = :topright,\n        markersize = 8,\n        size = (1200, 600))\n\n    # Annotate each point with the actual category name\n    for (i, col) in enumerate(eachcol(embedding_matrix))\n        if i <= length(feature_categories)\n            cat_name = string(feature_categories[i])\n            # Truncate long category names for readability\n            display_name = length(cat_name) > 10 ? cat_name[1:10] * \"...\" : cat_name\n            annotate!(p, col[1] + 0.02, col[2] + 0.02, text(display_name, :black, 5))\n        end\n    end\n\n    # Save the plot\n\n    # Display the plot\n    return p\nend;","category":"page"},{"location":"tutorials/entity_embeddings/notebook/#Generating-Embedding-Plots-for-Each-Categorical-Feature","page":"Entity Embeddings Tutorial","title":"Generating Embedding Plots for Each Categorical Feature","text":"","category":"section"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"Let's visualize the embedding space for each of our categorical features to understand what patterns the model learned.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Create separate plots for each categorical feature's embeddings\n\n# Plot 1: Category embeddings\nplot_categorical_embeddings(\n    :Category,\n    category_names[:Category],\n    mapping_matrices[:Category],\n);","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"Displayed embedding plot for: Category\n","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"(Image: Category Embeddings) Notice that pairs such as social and entertainment, shopping and finance, and comics and art are closer together than others.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Plot 2: Content Rating embeddings\nplot_categorical_embeddings(\n    Symbol(\"Content Rating\"),\n    category_names[Symbol(\"Content Rating\")],\n    mapping_matrices[Symbol(\"Content Rating\")],\n);","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"Displayed embedding plot for: Content Rating\n","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"(Image: Content Rating Embeddings) The Everyone category is positioned far from all others.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Plot 3: Genres embeddings\nplot_categorical_embeddings(:Genres, category_names[:Genres], mapping_matrices[:Genres]);","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"Displayed embedding plot for: Genres\n","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"(Image: Genres Embeddings) Here the results may be less interpretable; the idea is that for purposes of indetifying the rating, the model considered categories closer together as more similar.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Plot 4: Android Ver embeddings\nplot_categorical_embeddings(\n    Symbol(\"Android Ver\"),\n    category_names[Symbol(\"Android Ver\")],\n    mapping_matrices[Symbol(\"Android Ver\")],\n);","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"Displayed embedding plot for: Android Ver\n","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"(Image: Android Ver Embeddings) Clear patterns like close proximity between (7.1 and up) and, 7.0-7.1","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"# Plot 5: Type embeddings (if it exists in the mapping)\nplot_categorical_embeddings(:Type, sort(unique(df.Type)), mapping_matrices[:Type]);","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"Displayed embedding plot for: Type\n","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"(Image: Type Embeddings) Indeed, Free and Paid are too dissimilar.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"This demonstrates the power of entity embeddings as a modern approach to categorical feature encoding that goes beyond traditional methods like one-hot encoding or label encoding.","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"","category":"page"},{"location":"tutorials/entity_embeddings/notebook/","page":"Entity Embeddings Tutorial","title":"Entity Embeddings Tutorial","text":"This page was generated using Literate.jl.","category":"page"},{"location":"transformers/others/","page":"-","title":"-","text":"ransformers that operate on scientific types that are neither Finite nor Infinite.","category":"page"},{"location":"transformers/others/","page":"-","title":"-","text":"Transformer Brief Description\nUnivariateTimeTypeToContinuous Transform a vector of time type into continuous type","category":"page"},{"location":"transformers/others/","page":"-","title":"-","text":"MLJTransforms.UnivariateTimeTypeToContinuous","category":"page"},{"location":"transformers/others/#MLJTransforms.UnivariateTimeTypeToContinuous","page":"-","title":"MLJTransforms.UnivariateTimeTypeToContinuous","text":"UnivariateTimeTypeToContinuous\n\nA model type for constructing a single variable transformer that creates continuous representations of temporally typed data, based on unknown.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nUnivariateTimeTypeToContinuous = @load UnivariateTimeTypeToContinuous pkg=unknown\n\nDo model = UnivariateTimeTypeToContinuous() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in UnivariateTimeTypeToContinuous(zero_time=...).\n\nUse this model to convert vectors with a TimeType element type to vectors of Float64 type (Continuous element scitype).\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, x)\n\nwhere\n\nx: any abstract vector whose element type is a subtype of Dates.TimeType\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nzero_time: the time that is to correspond to 0.0 under transformations, with the type coinciding with the training data element type. If unspecified, the earliest time encountered in training is used.\nstep::Period=Hour(24): time interval to correspond to one unit under transformation\n\nOperations\n\ntransform(mach, xnew): apply the encoding inferred when mach was fit\n\nFitted parameters\n\nfitted_params(mach).fitresult is the tuple (zero_time, step) actually used in transformations, which may differ from the user-specified hyper-parameters.\n\nExample\n\nusing MLJ\nusing Dates\n\nx = [Date(2001, 1, 1) + Day(i) for i in 0:4]\n\nencoder = UnivariateTimeTypeToContinuous(zero_time=Date(2000, 1, 1),\n                                         step=Week(1))\n\nmach = machine(encoder, x)\nfit!(mach)\njulia> transform(mach, x)\n5-element Vector{Float64}:\n 52.285714285714285\n 52.42857142857143\n 52.57142857142857\n 52.714285714285715\n 52.857142\n\n\n\n\n\n","category":"type"},{"location":"transformers/all_transformers/#Summary-Table","page":"All Transformers","title":"Summary Table","text":"","category":"section"},{"location":"transformers/all_transformers/","page":"All Transformers","title":"All Transformers","text":"Transformer Brief Description\nStandardizer Transforming columns of numerical features by standardization\nUnivariateBoxCoxTransformer Apply BoxCox transformation given a single vector\nInteractionTransformer Transforming columns of numerical features to create new interaction features\nUnivariateDiscretizer Discretize a continuous vector into an ordered factor\nFillImputer Fill missing values of features belonging to any scientific type\nUnivariateTimeTypeToContinuous Transform a vector of time type into continuous type\nOneHotEncoder Encode categorical variables into one-hot vectors\nContinuousEncoder Adds type casting functionality to OnehotEncoder\nOrdinalEncoder Encode categorical variables into ordered integers\nFrequencyEncoder Encode categorical variables into their normalized or unormalized frequencies\nTargetEncoder Encode categorical variables into relevant target statistics\nDummyEncoder Encodes by comparing each level to the reference level, intercept being the cell mean of the reference group\nSumEncoder Encodes by comparing each level to the reference level, intercept being the grand mean\nHelmertEncoder Encodes by comparing levels of a variable with the mean of the subsequent levels of the variable\nForwardDifferenceEncoder Encodes by comparing adjacent levels of a variable (each level minus the next level)\nContrastEncoder Allows defining a custom contrast encoder via a contrast matrix\nHypothesisEncoder Allows defining a custom contrast encoder via a hypothesis matrix\nEntityEmbedders Encode categorical variables into dense embedding vectors\nCardinalityReducer Reduce cardinality of high cardinality categorical features by grouping infrequent categories\nMissingnessEncoder Encode missing values of categorical features into new values","category":"page"},{"location":"transformers/all_transformers/#All-Transformers","page":"All Transformers","title":"All Transformers","text":"","category":"section"},{"location":"transformers/all_transformers/","page":"All Transformers","title":"All Transformers","text":"MLJTransforms.Standardizer","category":"page"},{"location":"transformers/all_transformers/#MLJTransforms.Standardizer-transformers-all_transformers","page":"All Transformers","title":"MLJTransforms.Standardizer","text":"Standardizer\n\nA model type for constructing a standardizer, based on unknown.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nStandardizer = @load Standardizer pkg=unknown\n\nDo model = Standardizer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in Standardizer(features=...).\n\nUse this model to standardize (whiten) a Continuous vector, or relevant columns of a table. The rescalings applied by this transformer to new data are always those learned during the training phase, which are generally different from what would actually standardize the new data.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any Tables.jl compatible table or any abstract vector with Continuous element scitype (any abstract float vector). Only features in a table with Continuous scitype can be standardized; check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures: one of the following, with the behavior indicated below:\n[] (empty, the default): standardize all features (columns) having Continuous element scitype\nnon-empty vector of feature names (symbols): standardize only the Continuous features in the vector (if ignore=false) or Continuous features not named in the vector (ignore=true).\nfunction or other callable: standardize a feature if the callable returns true on its name. For example, Standardizer(features = name -> name in [:x1, :x3], ignore = true, count=true) has the same effect as Standardizer(features = [:x1, :x3], ignore = true, count=true), namely to standardize all Continuous and Count features, with the exception of :x1 and :x3.\nNote this behavior is further modified if the ordered_factor or count flags are set to true; see below\nignore=false: whether to ignore or standardize specified features, as explained above\nordered_factor=false: if true, standardize any OrderedFactor feature wherever a Continuous feature would be standardized, as described above\ncount=false: if true, standardize any Count feature wherever a Continuous feature would be standardized, as described above\n\nOperations\n\ntransform(mach, Xnew): return Xnew with relevant features standardized according to the rescalings learned during fitting of mach.\ninverse_transform(mach, Z): apply the inverse transformation to Z, so that inverse_transform(mach, transform(mach, Xnew)) is approximately the same as Xnew; unavailable if ordered_factor or count flags were set to true.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nfeatures_fit - the names of features that will be standardized\nmeans - the corresponding untransformed mean values\nstds - the corresponding untransformed standard deviations\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures_fit: the names of features that will be standardized\n\nExamples\n\nusing MLJ\n\nX = (ordinal1 = [1, 2, 3],\n     ordinal2 = coerce([:x, :y, :x], OrderedFactor),\n     ordinal3 = [10.0, 20.0, 30.0],\n     ordinal4 = [-20.0, -30.0, -40.0],\n     nominal = coerce([\"Your father\", \"he\", \"is\"], Multiclass));\n\njulia> schema(X)\n┌──────────┬──────────────────┐\n│ names    │ scitypes         │\n├──────────┼──────────────────┤\n│ ordinal1 │ Count            │\n│ ordinal2 │ OrderedFactor{2} │\n│ ordinal3 │ Continuous       │\n│ ordinal4 │ Continuous       │\n│ nominal  │ Multiclass{3}    │\n└──────────┴──────────────────┘\n\nstand1 = Standardizer();\n\njulia> transform(fit!(machine(stand1, X)), X)\n(ordinal1 = [1, 2, 3],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal3 = [-1.0, 0.0, 1.0],\n ordinal4 = [1.0, 0.0, -1.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\nstand2 = Standardizer(features=[:ordinal3, ], ignore=true, count=true);\n\njulia> transform(fit!(machine(stand2, X)), X)\n(ordinal1 = [-1.0, 0.0, 1.0],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal3 = [10.0, 20.0, 30.0],\n ordinal4 = [1.0, 0.0, -1.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\nSee also OneHotEncoder, ContinuousEncoder.\n\n\n\n\n\n","category":"type"},{"location":"transformers/all_transformers/","page":"All Transformers","title":"All Transformers","text":"MLJTransforms.InteractionTransformer","category":"page"},{"location":"transformers/all_transformers/#MLJTransforms.InteractionTransformer-transformers-all_transformers","page":"All Transformers","title":"MLJTransforms.InteractionTransformer","text":"InteractionTransformer\n\nA model type for constructing a interaction transformer, based on unknown.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nInteractionTransformer = @load InteractionTransformer pkg=unknown\n\nDo model = InteractionTransformer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in InteractionTransformer(order=...).\n\nGenerates all polynomial interaction terms up to the given order for the subset of chosen columns.  Any column that contains elements with scitype <:Infinite is a valid basis to generate interactions.  If features is not specified, all such columns with scitype <:Infinite in the table are used as a basis.\n\nIn MLJ or MLJBase, you can transform features X with the single call\n\ntransform(machine(model), X)\n\nSee also the example below.\n\nHyper-parameters\n\norder: Maximum order of interactions to be generated.\nfeatures: Restricts interations generation to those columns\n\nOperations\n\ntransform(machine(model), X): Generates polynomial interaction terms out of table X using the hyper-parameters specified in model.\n\nExample\n\nusing MLJ\n\nX = (\n    A = [1, 2, 3],\n    B = [4, 5, 6],\n    C = [7, 8, 9],\n    D = [\"x₁\", \"x₂\", \"x₃\"]\n)\nit = InteractionTransformer(order=3)\nmach = machine(it)\n\njulia> transform(mach, X)\n(A = [1, 2, 3],\n B = [4, 5, 6],\n C = [7, 8, 9],\n D = [\"x₁\", \"x₂\", \"x₃\"],\n A_B = [4, 10, 18],\n A_C = [7, 16, 27],\n B_C = [28, 40, 54],\n A_B_C = [28, 80, 162],)\n\nit = InteractionTransformer(order=2, features=[:A, :B])\nmach = machine(it)\n\njulia> transform(mach, X)\n(A = [1, 2, 3],\n B = [4, 5, 6],\n C = [7, 8, 9],\n D = [\"x₁\", \"x₂\", \"x₃\"],\n A_B = [4, 10, 18],)\n\n\n\n\n\n\n","category":"type"},{"location":"transformers/all_transformers/","page":"All Transformers","title":"All Transformers","text":"MLJTransforms.UnivariateDiscretizer","category":"page"},{"location":"transformers/all_transformers/#MLJTransforms.UnivariateDiscretizer-transformers-all_transformers","page":"All Transformers","title":"MLJTransforms.UnivariateDiscretizer","text":"UnivariateDiscretizer\n\nA model type for constructing a single variable discretizer, based on unknown.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nUnivariateDiscretizer = @load UnivariateDiscretizer pkg=unknown\n\nDo model = UnivariateDiscretizer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in UnivariateDiscretizer(n_classes=...).\n\nDiscretization converts a Continuous vector into an OrderedFactor vector. In particular, the output is a CategoricalVector (whose reference type is optimized).\n\nThe transformation is chosen so that the vector on which the transformer is fit has, in transformed form, an approximately uniform distribution of values. Specifically, if n_classes is the level of discretization, then 2*n_classes - 1 ordered quantiles are computed, the odd quantiles being used for transforming (discretization) and the even quantiles for inverse transforming.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, x)\n\nwhere\n\nx: any abstract vector with Continuous element scitype; check scitype with scitype(x).\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nn_classes: number of discrete classes in the output\n\nOperations\n\ntransform(mach, xnew): discretize xnew according to the discretization learned when fitting mach\ninverse_transform(mach, z): attempt to reconstruct from z a vector that transforms to give z\n\nFitted parameters\n\nThe fields of fitted_params(mach).fitesult include:\n\nodd_quantiles: quantiles used for transforming (length is n_classes - 1)\neven_quantiles: quantiles used for inverse transforming (length is n_classes)\n\nExample\n\nusing MLJ\nusing Random\nRandom.seed!(123)\n\ndiscretizer = UnivariateDiscretizer(n_classes=100)\nmach = machine(discretizer, randn(1000))\nfit!(mach)\n\njulia> x = rand(5)\n5-element Vector{Float64}:\n 0.8585244609846809\n 0.37541692370451396\n 0.6767070590395461\n 0.9208844241267105\n 0.7064611415680901\n\njulia> z = transform(mach, x)\n5-element CategoricalArrays.CategoricalArray{UInt8,1,UInt8}:\n 0x52\n 0x42\n 0x4d\n 0x54\n 0x4e\n\nx_approx = inverse_transform(mach, z)\njulia> x - x_approx\n5-element Vector{Float64}:\n 0.008224506144777322\n 0.012731354778359405\n 0.0056265330571125816\n 0.005738175684445124\n 0.006835652575801987\n\n\n\n\n\n","category":"type"},{"location":"transformers/all_transformers/","page":"All Transformers","title":"All Transformers","text":"MLJTransforms.FillImputer","category":"page"},{"location":"transformers/all_transformers/#MLJTransforms.FillImputer-transformers-all_transformers","page":"All Transformers","title":"MLJTransforms.FillImputer","text":"FillImputer\n\nA model type for constructing a fill imputer, based on unknown.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nFillImputer = @load FillImputer pkg=unknown\n\nDo model = FillImputer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in FillImputer(features=...).\n\nUse this model to impute missing values in tabular data. A fixed \"filler\" value is learned from the training data, one for each column of the table.\n\nFor imputing missing values in a vector, use UnivariateFillImputer instead.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose features each have element scitypes Union{Missing, T}, where T is a subtype of Continuous, Multiclass, OrderedFactor or Count. Check scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures: a vector of names of features (symbols) for which imputation is to be attempted; default is empty, which is interpreted as \"impute all\".\ncontinuous_fill: function or other callable to determine value to be imputed in the case of Continuous (abstract float) data; default is to apply median after skipping missing values\ncount_fill: function or other callable to determine value to be imputed in the case of Count (integer) data; default is to apply rounded median after skipping missing values\nfinite_fill: function or other callable to determine value to be imputed in the case of Multiclass or OrderedFactor data (categorical vectors); default is to apply mode after skipping missing values\n\nOperations\n\ntransform(mach, Xnew): return Xnew with missing values imputed with the fill values learned when fitting mach\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nfeatures_seen_in_fit: the names of features (features) encountered during training\nunivariate_transformer: the univariate model applied to determine   the fillers (it's fields contain the functions defining the filler computations)\nfiller_given_feature: dictionary of filler values, keyed on feature (column) names\n\nExamples\n\nusing MLJ\nimputer = FillImputer()\n\nX = (a = [1.0, 2.0, missing, 3.0, missing],\n     b = coerce([\"y\", \"n\", \"y\", missing, \"y\"], Multiclass),\n     c = [1, 1, 2, missing, 3])\n\nschema(X)\njulia> schema(X)\n┌───────┬───────────────────────────────┐\n│ names │ scitypes                      │\n├───────┼───────────────────────────────┤\n│ a     │ Union{Missing, Continuous}    │\n│ b     │ Union{Missing, Multiclass{2}} │\n│ c     │ Union{Missing, Count}         │\n└───────┴───────────────────────────────┘\n\nmach = machine(imputer, X)\nfit!(mach)\n\njulia> fitted_params(mach).filler_given_feature\n(filler = 2.0,)\n\njulia> fitted_params(mach).filler_given_feature\nDict{Symbol, Any} with 3 entries:\n  :a => 2.0\n  :b => \"y\"\n  :c => 2\n\njulia> transform(mach, X)\n(a = [1.0, 2.0, 2.0, 3.0, 2.0],\n b = CategoricalValue{String, UInt32}[\"y\", \"n\", \"y\", \"y\", \"y\"],\n c = [1, 1, 2, 2, 3],)\n\nSee also UnivariateFillImputer.\n\n\n\n\n\n","category":"type"},{"location":"transformers/all_transformers/","page":"All Transformers","title":"All Transformers","text":"MLJTransforms.UnivariateTimeTypeToContinuous","category":"page"},{"location":"transformers/all_transformers/#MLJTransforms.UnivariateTimeTypeToContinuous-transformers-all_transformers","page":"All Transformers","title":"MLJTransforms.UnivariateTimeTypeToContinuous","text":"UnivariateTimeTypeToContinuous\n\nA model type for constructing a single variable transformer that creates continuous representations of temporally typed data, based on unknown.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nUnivariateTimeTypeToContinuous = @load UnivariateTimeTypeToContinuous pkg=unknown\n\nDo model = UnivariateTimeTypeToContinuous() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in UnivariateTimeTypeToContinuous(zero_time=...).\n\nUse this model to convert vectors with a TimeType element type to vectors of Float64 type (Continuous element scitype).\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, x)\n\nwhere\n\nx: any abstract vector whose element type is a subtype of Dates.TimeType\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nzero_time: the time that is to correspond to 0.0 under transformations, with the type coinciding with the training data element type. If unspecified, the earliest time encountered in training is used.\nstep::Period=Hour(24): time interval to correspond to one unit under transformation\n\nOperations\n\ntransform(mach, xnew): apply the encoding inferred when mach was fit\n\nFitted parameters\n\nfitted_params(mach).fitresult is the tuple (zero_time, step) actually used in transformations, which may differ from the user-specified hyper-parameters.\n\nExample\n\nusing MLJ\nusing Dates\n\nx = [Date(2001, 1, 1) + Day(i) for i in 0:4]\n\nencoder = UnivariateTimeTypeToContinuous(zero_time=Date(2000, 1, 1),\n                                         step=Week(1))\n\nmach = machine(encoder, x)\nfit!(mach)\njulia> transform(mach, x)\n5-element Vector{Float64}:\n 52.285714285714285\n 52.42857142857143\n 52.57142857142857\n 52.714285714285715\n 52.857142\n\n\n\n\n\n","category":"type"},{"location":"transformers/all_transformers/","page":"All Transformers","title":"All Transformers","text":"MLJTransforms.OneHotEncoder","category":"page"},{"location":"transformers/all_transformers/#MLJTransforms.OneHotEncoder-transformers-all_transformers","page":"All Transformers","title":"MLJTransforms.OneHotEncoder","text":"OneHotEncoder\n\nA model type for constructing a one-hot encoder, based on unknown.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nOneHotEncoder = @load OneHotEncoder pkg=unknown\n\nDo model = OneHotEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in OneHotEncoder(features=...).\n\nUse this model to one-hot encode the Multiclass and OrderedFactor features (columns) of some table, leaving other columns unchanged.\n\nNew data to be transformed may lack features present in the fit data, but no new features can be present.\n\nWarning: This transformer assumes that levels(col) for any Multiclass or OrderedFactor column, col, is the same for training data and new data to be transformed.\n\nTo ensure all features are transformed into Continuous features, or dropped, use ContinuousEncoder instead.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any Tables.jl compatible table. Columns can be of mixed type but only those with element scitype Multiclass or OrderedFactor can be encoded. Check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures: a vector of symbols (feature names). If empty (default) then all Multiclass and OrderedFactor features are encoded. Otherwise, encoding is further restricted to the specified features (ignore=false) or the unspecified features (ignore=true). This default behavior can be modified by the ordered_factor flag.\nordered_factor=false: when true, OrderedFactor features are universally excluded\ndrop_last=true: whether to drop the column corresponding to the final class of encoded features. For example, a three-class feature is spawned into three new features if drop_last=false, but just two features otherwise.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nall_features: names of all features encountered in training\nfitted_levels_given_feature: dictionary of the levels associated with each feature encoded, keyed on the feature name\nref_name_pairs_given_feature: dictionary of pairs r => ftr (such as 0x00000001 => :grad__A) where r is a CategoricalArrays.jl reference integer representing a level, and ftr the corresponding new feature name; the dictionary is keyed on the names of features that are encoded\n\nReport\n\nThe fields of report(mach) are:\n\nfeatures_to_be_encoded: names of input features to be encoded\nnew_features: names of all output features\n\nExample\n\nusing MLJ\n\nX = (name=categorical([\"Danesh\", \"Lee\", \"Mary\", \"John\"]),\n     grade=categorical([\"A\", \"B\", \"A\", \"C\"], ordered=true),\n     height=[1.85, 1.67, 1.5, 1.67],\n     n_devices=[3, 2, 4, 3])\n\njulia> schema(X)\n┌───────────┬──────────────────┐\n│ names     │ scitypes         │\n├───────────┼──────────────────┤\n│ name      │ Multiclass{4}    │\n│ grade     │ OrderedFactor{3} │\n│ height    │ Continuous       │\n│ n_devices │ Count            │\n└───────────┴──────────────────┘\n\nhot = OneHotEncoder(drop_last=true)\nmach = fit!(machine(hot, X))\nW = transform(mach, X)\n\njulia> schema(W)\n┌──────────────┬────────────┐\n│ names        │ scitypes   │\n├──────────────┼────────────┤\n│ name__Danesh │ Continuous │\n│ name__John   │ Continuous │\n│ name__Lee    │ Continuous │\n│ grade__A     │ Continuous │\n│ grade__B     │ Continuous │\n│ height       │ Continuous │\n│ n_devices    │ Count      │\n└──────────────┴────────────┘\n\nSee also ContinuousEncoder.\n\n\n\n\n\n","category":"type"},{"location":"transformers/all_transformers/","page":"All Transformers","title":"All Transformers","text":"MLJTransforms.ContinuousEncoder","category":"page"},{"location":"transformers/all_transformers/#MLJTransforms.ContinuousEncoder-transformers-all_transformers","page":"All Transformers","title":"MLJTransforms.ContinuousEncoder","text":"ContinuousEncoder\n\nA model type for constructing a continuous encoder, based on unknown.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nContinuousEncoder = @load ContinuousEncoder pkg=unknown\n\nDo model = ContinuousEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ContinuousEncoder(drop_last=...).\n\nUse this model to arrange all features (features) of a table to have Continuous element scitype, by applying the following protocol to each feature ftr:\n\nIf ftr is already Continuous retain it.\nIf ftr is Multiclass, one-hot encode it.\nIf ftr is OrderedFactor, replace it with coerce(ftr, Continuous) (vector of floating point integers), unless ordered_factors=false is specified, in which case one-hot encode it.\nIf ftr is Count, replace it with coerce(ftr, Continuous).\nIf ftr has some other element scitype, or was not observed in fitting the encoder, drop it from the table.\n\nWarning: This transformer assumes that levels(col) for any Multiclass or OrderedFactor column, col, is the same for training data and new data to be transformed.\n\nTo selectively one-hot-encode categorical features (without dropping features) use OneHotEncoder instead.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X)\n\nwhere\n\nX: any Tables.jl compatible table. features can be of mixed type but only those with element scitype Multiclass or OrderedFactor can be encoded. Check column scitypes with schema(X).\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\ndrop_last=true: whether to drop the column corresponding to the final class of one-hot encoded features. For example, a three-class feature is spawned into three new features if drop_last=false, but two just features otherwise.\none_hot_ordered_factors=false: whether to one-hot any feature with OrderedFactor element scitype, or to instead coerce it directly to a (single) Continuous feature using the order\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nfeatures_to_keep: names of features that will not be dropped from the table\none_hot_encoder: the OneHotEncoder model instance for handling the one-hot encoding\none_hot_encoder_fitresult: the fitted parameters of the OneHotEncoder model\n\nReport\n\nfeatures_to_keep: names of input features that will not be dropped from the table\nnew_features: names of all output features\n\nExample\n\nX = (name=categorical([\"Danesh\", \"Lee\", \"Mary\", \"John\"]),\n     grade=categorical([\"A\", \"B\", \"A\", \"C\"], ordered=true),\n     height=[1.85, 1.67, 1.5, 1.67],\n     n_devices=[3, 2, 4, 3],\n     comments=[\"the force\", \"be\", \"with you\", \"too\"])\n\njulia> schema(X)\n┌───────────┬──────────────────┐\n│ names     │ scitypes         │\n├───────────┼──────────────────┤\n│ name      │ Multiclass{4}    │\n│ grade     │ OrderedFactor{3} │\n│ height    │ Continuous       │\n│ n_devices │ Count            │\n│ comments  │ Textual          │\n└───────────┴──────────────────┘\n\nencoder = ContinuousEncoder(drop_last=true)\nmach = fit!(machine(encoder, X))\nW = transform(mach, X)\n\njulia> schema(W)\n┌──────────────┬────────────┐\n│ names        │ scitypes   │\n├──────────────┼────────────┤\n│ name__Danesh │ Continuous │\n│ name__John   │ Continuous │\n│ name__Lee    │ Continuous │\n│ grade        │ Continuous │\n│ height       │ Continuous │\n│ n_devices    │ Continuous │\n└──────────────┴────────────┘\n\njulia> setdiff(schema(X).names, report(mach).features_to_keep) # dropped features\n1-element Vector{Symbol}:\n :comments\n\n\nSee also OneHotEncoder\n\n\n\n\n\n","category":"type"},{"location":"transformers/all_transformers/","page":"All Transformers","title":"All Transformers","text":"MLJTransforms.OrdinalEncoder","category":"page"},{"location":"transformers/all_transformers/#MLJTransforms.OrdinalEncoder-transformers-all_transformers","page":"All Transformers","title":"MLJTransforms.OrdinalEncoder","text":"OrdinalEncoder\n\nA model type for constructing a ordinal encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nOrdinalEncoder = @load OrdinalEncoder pkg=MLJTransforms\n\nDo model = OrdinalEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in OrdinalEncoder(features=...).\n\nOrdinalEncoder implements ordinal encoding which replaces the categorical values in the specified     categorical features with integers (ordered arbitrarily). This will create an implicit ordering between     categories which may not be a proper modelling assumption.\n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to   check scitypes. \n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\noutput_type: The numerical concrete type of the encoded features. Default is Float32.\n\nOperations\n\ntransform(mach, Xnew): Apply ordinal encoding to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and   return the new table.   Features that are neither Multiclass nor OrderedFactor  are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nindex_given_feat_level: A dictionary that maps each level for each column in a subset of the categorical features of X into an integer. \n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nusing MLJ\n\n# Define categorical features\nA = [\"g\", \"b\", \"g\", \"r\", \"r\",]  \nB = [1.0, 2.0, 3.0, 4.0, 5.0,]\nC = [\"f\", \"f\", \"f\", \"m\", \"f\",]  \nD = [true, false, true, false, true,]\nE = [1, 2, 3, 4, 5,]\n\n# Combine into a named tuple\nX = (A = A, B = B, C = C, D = D, E = E)\n\n# Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Continuous,\n:C => Multiclass,\n:D => Multiclass,\n:E => OrderedFactor,\n)\n\n# Check scitype coercion:\nschema(X)\n\nencoder = OrdinalEncoder(ordered_factor = false)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia > Xnew\n    (A = [2, 1, 2, 3, 3],\n    B = [1.0, 2.0, 3.0, 4.0, 5.0],\n    C = [1, 1, 1, 2, 1],\n    D = [2, 1, 2, 1, 2],\n    E = CategoricalArrays.CategoricalValue{Int64, UInt32}[1, 2, 3, 4, 5],)\n\nSee also TargetEncoder\n\n\n\n\n\n","category":"type"},{"location":"transformers/all_transformers/","page":"All Transformers","title":"All Transformers","text":"MLJTransforms.FrequencyEncoder","category":"page"},{"location":"transformers/all_transformers/#MLJTransforms.FrequencyEncoder-transformers-all_transformers","page":"All Transformers","title":"MLJTransforms.FrequencyEncoder","text":"FrequencyEncoder\n\nA model type for constructing a frequency encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nFrequencyEncoder = @load FrequencyEncoder pkg=MLJTransforms\n\nDo model = FrequencyEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in FrequencyEncoder(features=...).\n\nFrequencyEncoder implements frequency encoding which replaces the categorical values in the specified     categorical features with their (normalized or raw) frequencies of occurrence in the dataset. \n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to   check scitypes. \n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\nnormalize=false: Whether to use normalized frequencies that sum to 1 over category values or to use raw counts.\noutput_type=Float32: The type of the output values. The default is Float32, but you can set it to Float64 or any other type that can hold the frequency values.\n\nOperations\n\ntransform(mach, Xnew): Apply frequency encoding to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and   return the new table.   Features that are neither Multiclass nor OrderedFactor  are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nstatistic_given_feat_val: A dictionary that maps each level for each column in a subset of the categorical features of X into its frequency.\n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nusing MLJ\n\n# Define categorical features\nA = [\"g\", \"b\", \"g\", \"r\", \"r\",]  \nB = [1.0, 2.0, 3.0, 4.0, 5.0,]\nC = [\"f\", \"f\", \"f\", \"m\", \"f\",]  \nD = [true, false, true, false, true,]\nE = [1, 2, 3, 4, 5,]\n\n# Combine into a named tuple\nX = (A = A, B = B, C = C, D = D, E = E)\n\n# Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Continuous,\n:C => Multiclass,\n:D => Multiclass,\n:E => OrderedFactor,\n)\n\n# Check scitype coercions:\nschema(X)\n\nencoder = FrequencyEncoder(ordered_factor = false, normalize=true)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia > Xnew\n    (A = [2, 1, 2, 2, 2],\n    B = [1.0, 2.0, 3.0, 4.0, 5.0],\n    C = [4, 4, 4, 1, 4],\n    D = [3, 2, 3, 2, 3],\n    E = CategoricalArrays.CategoricalValue{Int64, UInt32}[1, 2, 3, 4, 5],)\n\nSee also TargetEncoder\n\n\n\n\n\n","category":"type"},{"location":"transformers/all_transformers/","page":"All Transformers","title":"All Transformers","text":"MLJTransforms.TargetEncoder","category":"page"},{"location":"transformers/all_transformers/#MLJTransforms.TargetEncoder-transformers-all_transformers","page":"All Transformers","title":"MLJTransforms.TargetEncoder","text":"TargetEncoder\n\nA model type for constructing a target encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nTargetEncoder = @load TargetEncoder pkg=MLJTransforms\n\nDo model = TargetEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in TargetEncoder(features=...).\n\nTargetEncoder implements target encoding as defined in [1] to encode categorical variables      into continuous ones using statistics from the target variable.\n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance model to data with\n\nmach = machine(model, X, y)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to   check scitypes. \n\ny is the target, which can be any AbstractVector whose element scitype is Continuous or Count for regression problems and  Multiclass or OrderedFactor for classification problems; check the scitype with schema(y)\n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\nλ: Shrinkage hyperparameter used to mix between posterior and prior statistics as described in [1]\nm: An integer hyperparameter to compute shrinkage as described in [1]. If m=:auto then m will be computed using\n\nempirical Bayes estimation as described in [1]\n\nOperations\n\ntransform(mach, Xnew): Apply target encoding to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and   return the new table.   Features that are neither Multiclass nor OrderedFactor  are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\ntask: Whether the task is Classification or Regression\ny_statistic_given_feat_level: A dictionary with the necessary statistics to encode each categorical feature. It maps each    level in each categorical feature to a statistic computed over the target.\n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nusing MLJ\n\n# Define categorical features\nA = [\"g\", \"b\", \"g\", \"r\", \"r\",]  \nB = [1.0, 2.0, 3.0, 4.0, 5.0,]\nC = [\"f\", \"f\", \"f\", \"m\", \"f\",]  \nD = [true, false, true, false, true,]\nE = [1, 2, 3, 4, 5,]\n\n# Define the target variable \ny = [\"c1\", \"c2\", \"c3\", \"c1\", \"c2\",]\n\n# Combine into a named tuple\nX = (A = A, B = B, C = C, D = D, E = E)\n\n# Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Continuous,\n:C => Multiclass,\n:D => Multiclass,\n:E => OrderedFactor,\n)\ny = coerce(y, Multiclass)\n\nencoder = TargetEncoder(ordered_factor = false, lambda = 1.0, m = 0,)\nmach = fit!(machine(encoder, X, y))\nXnew = transform(mach, X)\n\njulia > schema(Xnew)\n┌───────┬──────────────────┬─────────────────────────────────┐\n│ names │ scitypes         │ types                           │\n├───────┼──────────────────┼─────────────────────────────────┤\n│ A_1   │ Continuous       │ Float64                         │\n│ A_2   │ Continuous       │ Float64                         │\n│ A_3   │ Continuous       │ Float64                         │\n│ B     │ Continuous       │ Float64                         │\n│ C_1   │ Continuous       │ Float64                         │\n│ C_2   │ Continuous       │ Float64                         │\n│ C_3   │ Continuous       │ Float64                         │\n│ D_1   │ Continuous       │ Float64                         │\n│ D_2   │ Continuous       │ Float64                         │\n│ D_3   │ Continuous       │ Float64                         │\n│ E     │ OrderedFactor{5} │ CategoricalValue{Int64, UInt32} │\n└───────┴──────────────────┴─────────────────────────────────┘\n\nReference\n\n[1] Micci-Barreca, Daniele.      “A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems”      SIGKDD Explor. Newsl. 3, 1 (July 2001), 27–32.\n\nSee also OneHotEncoder\n\n\n\n\n\n","category":"type"},{"location":"transformers/all_transformers/","page":"All Transformers","title":"All Transformers","text":"MLJTransforms.ContrastEncoder","category":"page"},{"location":"transformers/all_transformers/#MLJTransforms.ContrastEncoder-transformers-all_transformers","page":"All Transformers","title":"MLJTransforms.ContrastEncoder","text":"ContrastEncoder\n\nA model type for constructing a contrast encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nContrastEncoder = @load ContrastEncoder pkg=MLJTransforms\n\nDo model = ContrastEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ContrastEncoder(features=...).\n\nContrastEncoder implements the following contrast encoding methods for  categorical features: dummy, sum, backward/forward difference, and Helmert coding.  More generally, users can specify a custom contrast or hypothesis matrix, and each feature  can be encoded using a different method.\n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to   check scitypes. \n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\nmode=:dummy: The type of encoding to use. Can be one of :contrast, :dummy, :sum, :backward_diff, :forward_diff, :helmert or :hypothesis.\n\nIf ignore=false (features to be encoded are listed explictly in features), then this can be a vector of the same length as features to specify a different contrast encoding scheme for each feature\n\nbuildmatrix=nothing: A function or other callable with signature buildmatrix(colname, k), \n\nwhere colname is the name of the feature levels and k is it's length, and which returns contrast or  hypothesis matrix with row/column ordering consistent with the ordering of levels(col). Only relevant if mode is :contrast or :hypothesis.\n\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\n\nOperations\n\ntransform(mach, Xnew): Apply contrast encoding to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and   return the new table. Features that are neither Multiclass nor OrderedFactor  are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nvector_given_value_given_feature: A dictionary that maps each level for each column in a subset of the categorical features of X into its frequency.\n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nusing MLJ\n\n# Define categorical dataset\nX = (\n    name   = categorical([\"Ben\", \"John\", \"Mary\", \"John\"]),\n    height = [1.85, 1.67, 1.5, 1.67],\n    favnum = categorical([7, 5, 10, 1]),\n    age    = [23, 23, 14, 23],\n)\n\n# Check scitype coercions:\nschema(X)\n\nencoder =  ContrastEncoder(\n    features = [:name, :favnum],\n    ignore = false, \n    mode = [:dummy, :helmert],\n)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia > Xnew\n    (name_John = [1.0, 0.0, 0.0, 0.0],\n    name_Mary = [0.0, 1.0, 0.0, 1.0],\n    height = [1.85, 1.67, 1.5, 1.67],\n    favnum_5 = [0.0, 1.0, 0.0, -1.0],\n    favnum_7 = [2.0, -1.0, 0.0, -1.0],\n    favnum_10 = [-1.0, -1.0, 3.0, -1.0],\n    age = [23, 23, 14, 23],)\n\nSee also OneHotEncoder\n\n\n\n\n\n","category":"type"},{"location":"transformers/all_transformers/","page":"All Transformers","title":"All Transformers","text":"MLJTransforms.CardinalityReducer","category":"page"},{"location":"transformers/all_transformers/#MLJTransforms.CardinalityReducer-transformers-all_transformers","page":"All Transformers","title":"MLJTransforms.CardinalityReducer","text":"CardinalityReducer\n\nA model type for constructing a cardinality reducer, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nCardinalityReducer = @load CardinalityReducer pkg=MLJTransforms\n\nDo model = CardinalityReducer() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in CardinalityReducer(features=...).\n\nCardinalityReducer maps any level of a categorical feature that occurs with frequency < min_frequency into a new level (e.g., \"Other\"). This is useful when some categorical features have high cardinality and many levels are infrequent. This assumes that the categorical features have raw types that are in Union{AbstractString, Char, Number}.\n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to   check scitypes. \n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\nmin_frequency::Real=3: Any level of a categorical feature that occurs with frequency < min_frequency will be mapped to a new level. Could be\n\nan integer or a float which decides whether raw counts or normalized frequencies are used.\n\nlabel_for_infrequent::Dict{<:Type, <:Any}()= Dict( AbstractString => \"Other\", Char => 'O', ): A\n\ndictionary where the possible values for keys are the types in Char, AbstractString, and Number and each value signifies the new level to map into given a column raw super type. By default, if the raw type of the column subtypes AbstractString then the new value is \"Other\" and if the raw type subtypes Char then the new value is 'O' and if the raw type subtypes Number then the new value is the lowest value in the column - 1.\n\nOperations\n\ntransform(mach, Xnew): Apply cardinality reduction to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and   return the new table.   Features that are neither Multiclass nor OrderedFactor  are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nnew_cat_given_col_val: A dictionary that maps each level in a   categorical feature to a new level (either itself or the new level specified in label_for_infrequent)\n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nimport StatsBase.proportionmap\nusing MLJ\n\n# Define categorical features\nA = [ [\"a\" for i in 1:100]..., \"b\", \"b\", \"b\", \"c\", \"d\"]\nB = [ [0 for i in 1:100]..., 1, 2, 3, 4, 4]\n\n# Combine into a named tuple\nX = (A = A, B = B)\n\n# Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Multiclass\n)\n\nencoder = CardinalityReducer(ordered_factor = false, min_frequency=3)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia> proportionmap(Xnew.A)\nDict{CategoricalArrays.CategoricalValue{String, UInt32}, Float64} with 3 entries:\n  \"Other\" => 0.0190476\n  \"b\"     => 0.0285714\n  \"a\"     => 0.952381\n\njulia> proportionmap(Xnew.B)\nDict{CategoricalArrays.CategoricalValue{Int64, UInt32}, Float64} with 2 entries:\n  0  => 0.952381\n  -1 => 0.047619\n\nSee also FrequencyEncoder\n\n\n\n\n\n","category":"type"},{"location":"transformers/all_transformers/","page":"All Transformers","title":"All Transformers","text":"MLJTransforms.MissingnessEncoder","category":"page"},{"location":"transformers/all_transformers/#MLJTransforms.MissingnessEncoder-transformers-all_transformers","page":"All Transformers","title":"MLJTransforms.MissingnessEncoder","text":"MissingnessEncoder\n\nA model type for constructing a missingness encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMissingnessEncoder = @load MissingnessEncoder pkg=MLJTransforms\n\nDo model = MissingnessEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MissingnessEncoder(features=...).\n\nMissingnessEncoder maps any missing level of a categorical feature into a new level (e.g., \"Missing\").  By this, missingness will be treated as a new level by any subsequent model. This assumes that the categorical features have raw types that are in Char, AbstractString, and Number.\n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to   check scitypes. \n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\nlabel_for_missing::Dict{<:Type, <:Any}()= Dict( AbstractString => \"missing\", Char => 'm', ): A\n\ndictionary where the possible values for keys are the types in Char, AbstractString, and Number and where each value signifies the new level to map into given a column raw super type. By default, if the raw type of the column subtypes AbstractString then missing values will be replaced with \"missing\" and if the raw type subtypes Char then the new value is 'm' and if the raw type subtypes Number then the new value is the lowest value in the column - 1.\n\nOperations\n\ntransform(mach, Xnew): Apply cardinality reduction to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and   return the new table.   Features that are neither Multiclass nor OrderedFactor  are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nlabel_for_missing_given_feature: A dictionary that for each column, maps missing into some value according to label_for_missing\n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nimport StatsBase.proportionmap\nusing MLJ\n\n# Define a table with missing values\nXm = (\n    A = categorical([\"Ben\", \"John\", missing, missing, \"Mary\", \"John\", missing]),\n    B = [1.85, 1.67, missing, missing, 1.5, 1.67, missing],\n    C= categorical([7, 5, missing, missing, 10, 0, missing]),\n    D = [23, 23, 44, 66, 14, 23, 11],\n    E = categorical([missing, 'g', 'r', missing, 'r', 'g', 'p'])\n)\n\nencoder = MissingnessEncoder()\nmach = fit!(machine(encoder, Xm))\nXnew = transform(mach, Xm)\n\njulia> Xnew\n(A = [\"Ben\", \"John\", \"missing\", \"missing\", \"Mary\", \"John\", \"missing\"],\n B = Union{Missing, Float64}[1.85, 1.67, missing, missing, 1.5, 1.67, missing],\n C = [7, 5, -1, -1, 10, 0, -1],\n D = [23, 23, 44, 66, 14, 23, 11],\n E = ['m', 'g', 'r', 'm', 'r', 'g', 'p'],)\n\n\nSee also CardinalityReducer\n\n\n\n\n\n","category":"type"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"EditURL = \"notebook.jl\"","category":"page"},{"location":"tutorials/standardization/notebook/#Effects-of-Feature-Standardization-on-Model-Performance","page":"Standardization Impact","title":"Effects of Feature Standardization on Model Performance","text":"","category":"section"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"Welcome to this tutorial on feature standardization in machine learning! In this tutorial, we'll explore how standardizing features can significantly impact the performance of different machine learning models.","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"We'll compare Logistic Regression and Support Vector Machine (SVM) models, both with and without feature standardization. This will help us understand when and why preprocessing is important for model performance.","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"This demonstration is available as a Jupyter notebook or julia script here.","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"using Pkg\nPkg.activate(@__DIR__);\nPkg.instantiate();","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"  Activating project at `~/Documents/GitHub/MLJTransforms/docs/src/tutorials/standardization`\n","category":"page"},{"location":"tutorials/standardization/notebook/#Setup","page":"Standardization Impact","title":"Setup","text":"","category":"section"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"First, let's make sure we're using a compatible Julia version. This code was tested with Julia 1.10. Let's import all the packages we'll need for this tutorial.","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"# Load the necessary packages\nusing MLJ                   # Core MLJ framework\nusing LIBSVM                # For Support Vector Machine\nusing DataFrames            # For displaying results\nusing RDatasets             # To load sample datasets\nusing Random                # For reproducibility\nusing ScientificTypes       # For proper data typing\nusing Plots                 # For visualizations\nusing MLJLinearModels       # For Logistic Regression","category":"page"},{"location":"tutorials/standardization/notebook/#Data-Preparation","page":"Standardization Impact","title":"Data Preparation","text":"","category":"section"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"Let's load the Pima Indians Diabetes Dataset. This is a classic dataset for binary classification, where we predict diabetes status based on various health metrics.","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"The interesting thing about this dataset is that different features have very different scales. We'll artificially exaggerate this by adding a large constant to the glucose values.","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"# Load the dataset and modify it to have extreme scale differences\ndf = RDatasets.dataset(\"MASS\", \"Pima.tr\")\ndf.Glu .+= 10000.0;  # Artificially increase the scale of glucose values","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"Let's examine the first few rows of our dataset:","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"first(df, 5)","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"<div><div style = \"float: left;\"><span>5×8 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">NPreg</th><th style = \"text-align: left;\">Glu</th><th style = \"text-align: left;\">BP</th><th style = \"text-align: left;\">Skin</th><th style = \"text-align: left;\">BMI</th><th style = \"text-align: left;\">Ped</th><th style = \"text-align: left;\">Age</th><th style = \"text-align: left;\">Type</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Int32\" style = \"text-align: left;\">Int32</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int32\" style = \"text-align: left;\">Int32</th><th title = \"Int32\" style = \"text-align: left;\">Int32</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int32\" style = \"text-align: left;\">Int32</th><th title = \"CategoricalArrays.CategoricalValue{String, UInt8}\" style = \"text-align: left;\">Cat…</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">10086.0</td><td style = \"text-align: right;\">68</td><td style = \"text-align: right;\">28</td><td style = \"text-align: right;\">30.2</td><td style = \"text-align: right;\">0.364</td><td style = \"text-align: right;\">24</td><td style = \"text-align: left;\">No</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">7</td><td style = \"text-align: right;\">10195.0</td><td style = \"text-align: right;\">70</td><td style = \"text-align: right;\">33</td><td style = \"text-align: right;\">25.1</td><td style = \"text-align: right;\">0.163</td><td style = \"text-align: right;\">55</td><td style = \"text-align: left;\">Yes</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">5</td><td style = \"text-align: right;\">10077.0</td><td style = \"text-align: right;\">82</td><td style = \"text-align: right;\">41</td><td style = \"text-align: right;\">35.8</td><td style = \"text-align: right;\">0.156</td><td style = \"text-align: right;\">35</td><td style = \"text-align: left;\">No</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">10165.0</td><td style = \"text-align: right;\">76</td><td style = \"text-align: right;\">43</td><td style = \"text-align: right;\">47.9</td><td style = \"text-align: right;\">0.259</td><td style = \"text-align: right;\">26</td><td style = \"text-align: left;\">No</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">10107.0</td><td style = \"text-align: right;\">60</td><td style = \"text-align: right;\">25</td><td style = \"text-align: right;\">26.4</td><td style = \"text-align: right;\">0.133</td><td style = \"text-align: right;\">23</td><td style = \"text-align: left;\">No</td></tr></tbody></table></div>","category":"page"},{"location":"tutorials/standardization/notebook/#Data-Type-Conversion","page":"Standardization Impact","title":"Data Type Conversion","text":"","category":"section"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"In MLJ, it's important to ensure that our data has the correct scientific types. This helps the framework understand how to properly handle each column.","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"We'll convert our columns to their appropriate types:","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"Count for discrete count data\nContinuous for continuous numerical data\nMulticlass for our target variable","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"# Coerce columns to the right scientific types\ndf = coerce(df,\n    :NPreg => Count,      # Number of pregnancies is a count\n    :Glu => Continuous,   # Glucose level is continuous\n    :BP => Continuous,    # Blood pressure is continuous\n    :Skin => Continuous,  # Skin thickness is continuous\n    :BMI => Continuous,   # Body mass index is continuous\n    :Ped => Continuous,   # Diabetes pedigree is continuous\n    :Age => Continuous,   # Age is continuous\n    :Type => Multiclass,  # Diabetes status is our target (Yes/No)\n);","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"Let's verify that our schema looks correct:","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"ScientificTypes.schema(df)","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"┌───────┬───────────────┬─────────────────────────────────┐\n│ names │ scitypes      │ types                           │\n├───────┼───────────────┼─────────────────────────────────┤\n│ NPreg │ Count         │ Int32                           │\n│ Glu   │ Continuous    │ Float64                         │\n│ BP    │ Continuous    │ Float64                         │\n│ Skin  │ Continuous    │ Float64                         │\n│ BMI   │ Continuous    │ Float64                         │\n│ Ped   │ Continuous    │ Float64                         │\n│ Age   │ Continuous    │ Float64                         │\n│ Type  │ Multiclass{2} │ CategoricalValue{String, UInt8} │\n└───────┴───────────────┴─────────────────────────────────┘\n","category":"page"},{"location":"tutorials/standardization/notebook/#Feature-Extraction-and-Data-Splitting","page":"Standardization Impact","title":"Feature Extraction and Data Splitting","text":"","category":"section"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"Now we'll separate our features from our target variable. In MLJ, this is done with the unpack function.","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"# Unpack features (X) and target (y)\ny, X = unpack(df, ==(:Type); rng = 123);","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"Next, we'll split our data into training and testing sets. We'll use 70% for training and 30% for testing.","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"# Split data into train and test sets\ntrain, test = partition(eachindex(y), 0.7, shuffle = true, rng = 123);","category":"page"},{"location":"tutorials/standardization/notebook/#Model-Setup","page":"Standardization Impact","title":"Model Setup","text":"","category":"section"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"We'll compare two different models:","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"Logistic Regression: A linear model good for binary classification\nSupport Vector Machine (SVM): A powerful non-linear classifier","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"For each model, we'll create two versions:","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"One without standardization\nOne with standardization","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"The Standardizer transformer will rescale our features to have mean 0 and standard deviation 1.","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"# Load our models from their respective packages\nlogreg = @load LogisticClassifier pkg = MLJLinearModels\nsvm = @load SVC pkg = LIBSVM\nstand = Standardizer()  # This is our standardization transformer\n\n# Create pipelines for each model variant\nlogreg_pipe = logreg()  # Plain logistic regression\nlogreg_std_pipe = Pipeline(stand, logreg())  # Logistic regression with standardization\nsvm_pipe = svm()  # Plain SVM\nsvm_std_pipe = Pipeline(stand, svm())  # SVM with standardization","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"DeterministicPipeline(\n  standardizer = Standardizer(\n        features = Symbol[], \n        ignore = false, \n        ordered_factor = false, \n        count = false), \n  svc = SVC(\n        kernel = LIBSVM.Kernel.RadialBasis, \n        gamma = 0.0, \n        cost = 1.0, \n        cachesize = 200.0, \n        degree = 3, \n        coef0 = 0.0, \n        tolerance = 0.001, \n        shrinking = true), \n  cache = true)","category":"page"},{"location":"tutorials/standardization/notebook/#Model-Evaluation","page":"Standardization Impact","title":"Model Evaluation","text":"","category":"section"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"Let's set up a vector of our models so we can evaluate them all using the same process. For each model, we'll store its name and the corresponding pipeline.","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"# Create a list of models to evaluate\nmodels = [\n    (\"Logistic Regression\", logreg_pipe),\n    (\"Logistic Regression (standardized)\", logreg_std_pipe),\n    (\"SVM\", svm_pipe),\n    (\"SVM (standardized)\", svm_std_pipe),\n]","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"4-element Vector{Tuple{String, MLJModelInterface.Supervised}}:\n (\"Logistic Regression\", LogisticClassifier(lambda = 2.220446049250313e-16, …))\n (\"Logistic Regression (standardized)\", ProbabilisticPipeline(standardizer = Standardizer(features = Symbol[], …), …))\n (\"SVM\", SVC(kernel = RadialBasis, …))\n (\"SVM (standardized)\", DeterministicPipeline(standardizer = Standardizer(features = Symbol[], …), …))","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"Now we'll loop through each model, train it, make predictions, and calculate accuracy. This will help us compare how standardization affects each model's performance.","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"# Train and evaluate each model\nresults = DataFrame(model = String[], accuracy = Float64[])\nfor (name, model) in models\n    # Create a machine learning model\n    mach = machine(model, X, y)\n\n    # Train the model on the training data\n    MLJ.fit!(mach, rows = train)\n\n    # Make predictions on the test data\n    # Note: Logistic regression returns probabilities, so we need to get the mode\n    yhat =\n        occursin(\"Logistic Regression\", name) ?\n        MLJ.predict_mode(mach, rows = test) :  # Get most likely class for logistic regression\n        MLJ.predict(mach, rows = test)         # SVM directly predicts the class\n\n    # Calculate accuracy\n    acc = accuracy(yhat, y[test])\n\n    # Store the results\n    push!(results, (name, acc))\nend","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"┌ Warning: The number and/or types of data arguments do not match what the specified model\n│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n│ \n│ Run `@doc MLJLinearModels.LogisticClassifier` to learn more about your model's requirements.\n│ \n│ Commonly, but non exclusively, supervised models are constructed using the syntax\n│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n│ sample or class weights.\n│ \n│ In general, data in `machine(model, data...)` is expected to satisfy\n│ \n│     scitype(data) <: MLJ.fit_data_scitype(model)\n│ \n│ In the present case:\n│ \n│ scitype(data) = Tuple{ScientificTypesBase.Table{Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Count}}}, AbstractVector{ScientificTypesBase.Multiclass{2}}}\n│ \n│ fit_data_scitype(model) = Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}\n└ @ MLJBase ~/.julia/packages/MLJBase/F1Eh6/src/machines.jl:237\n[ Info: Training machine(LogisticClassifier(lambda = 2.220446049250313e-16, …), …).\n┌ Info: Solver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n│   optim_options: Optim.Options{Float64, Nothing}\n└   lbfgs_options: @NamedTuple{} NamedTuple()\n[ Info: Training machine(ProbabilisticPipeline(standardizer = Standardizer(features = Symbol[], …), …), …).\n[ Info: Training machine(:standardizer, …).\n[ Info: Training machine(:logistic_classifier, …).\n┌ Info: Solver: MLJLinearModels.LBFGS{Optim.Options{Float64, Nothing}, @NamedTuple{}}\n│   optim_options: Optim.Options{Float64, Nothing}\n└   lbfgs_options: @NamedTuple{} NamedTuple()\n┌ Warning: The number and/or types of data arguments do not match what the specified model\n│ supports. Suppress this type check by specifying `scitype_check_level=0`.\n│ \n│ Run `@doc LIBSVM.SVC` to learn more about your model's requirements.\n│ \n│ Commonly, but non exclusively, supervised models are constructed using the syntax\n│ `machine(model, X, y)` or `machine(model, X, y, w)` while most other models are\n│ constructed with `machine(model, X)`.  Here `X` are features, `y` a target, and `w`\n│ sample or class weights.\n│ \n│ In general, data in `machine(model, data...)` is expected to satisfy\n│ \n│     scitype(data) <: MLJ.fit_data_scitype(model)\n│ \n│ In the present case:\n│ \n│ scitype(data) = Tuple{ScientificTypesBase.Table{Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Count}}}, AbstractVector{ScientificTypesBase.Multiclass{2}}}\n│ \n│ fit_data_scitype(model) = Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}, Any}}\n└ @ MLJBase ~/.julia/packages/MLJBase/F1Eh6/src/machines.jl:237\n[ Info: Training machine(SVC(kernel = RadialBasis, …), …).\n[ Info: Training machine(DeterministicPipeline(standardizer = Standardizer(features = Symbol[], …), …), …).\n[ Info: Training machine(:standardizer, …).\n[ Info: Training machine(:svc, …).\n","category":"page"},{"location":"tutorials/standardization/notebook/#Results-Visualization","page":"Standardization Impact","title":"Results Visualization","text":"","category":"section"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"Finally, let's visualize our results to see the impact of standardization. We'll create a bar chart comparing the accuracy of each model.","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"# Create a bar chart of model performance\np = bar(\n    results.model,\n    results.accuracy,\n    xlabel = \"Model\",\n    ylabel = \"Accuracy\",\n    title = \"Model Accuracy Comparison\",\n    legend = false,\n    bar_width = 0.6,\n    ylims = (0.5, 0.7),\n    xrotation = 17,\n);","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"Save the plot","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"(Image: Model Accuracy Comparison)","category":"page"},{"location":"tutorials/standardization/notebook/#Conclusion","page":"Standardization Impact","title":"Conclusion","text":"","category":"section"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"From this tutorial, we can clearly see that standardization has a dramatic impact on model performance.","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"Looking at the results:","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"Logistic Regression: Without standardization, it achieves only ~57% accuracy. With standardization, its performance jumps dramatically to ~68% accuracy – the best performance among all models.\nSVM: The baseline SVM achieves ~62% accuracy. When standardized, it improves to ~65% accuracy, which is a significant boost but not as dramatic as what we see with logistic regression.","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"Try this approach with other datasets and models to further explore the effects of standardization!","category":"page"},{"location":"tutorials/standardization/notebook/#Further-Resources","page":"Standardization Impact","title":"Further Resources","text":"","category":"section"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"MLJTransforms Documentation\nScientific Types in MLJ\nFeature Preprocessing in MLJ","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"","category":"page"},{"location":"tutorials/standardization/notebook/","page":"Standardization Impact","title":"Standardization Impact","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"EditURL = \"notebook.jl\"","category":"page"},{"location":"tutorials/wine_example/notebook/#Wine-Quality-Prediction:-Comparing-Categorical-Encoders","page":"Wine Quality Prediction","title":"Wine Quality Prediction: Comparing Categorical Encoders","text":"","category":"section"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"Julia version is assumed to be 1.10.*","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"This demonstration is available as a Jupyter notebook or julia script (as well as the dataset) here.","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"This tutorial compares different categorical encoding approaches on wine quality prediction. We'll test OneHot, Frequency, and Cardinality Reduction encoders with CatBoost regression.","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"Why compare encoders? Categorical variables with many levels (like wine varieties) can create high-dimensional sparse features. Different encoding strategies handle this challenge differently, affecting both model performance and training speed.","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"Note: we do not endorse drinking alcohol, this tutorial is purely for educational purposes.","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"using Pkg;\nPkg.activate(@__DIR__);\n\nusing MLJ, MLJTransforms, DataFrames, ScientificTypes\nusing Random, CSV, StatsBase, Plots;","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"  Activating project at `~/Documents/GitHub/MLJTransforms/docs/src/tutorials/wine_example`\n","category":"page"},{"location":"tutorials/wine_example/notebook/#Load-and-Prepare-Data","page":"Wine Quality Prediction","title":"Load and Prepare Data","text":"","category":"section"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"Load the wine dataset and take a sample for faster computation. The dataset contains wine reviews with categorical features like variety, winery, and region:","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"df = CSV.read(\"./clean_wine.csv\", DataFrame)\n\nfirst(df, 5)","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"<div><div style = \"float: left;\"><span>5×8 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">country</th><th style = \"text-align: left;\">points</th><th style = \"text-align: left;\">price</th><th style = \"text-align: left;\">province</th><th style = \"text-align: left;\">region_1</th><th style = \"text-align: left;\">region_2</th><th style = \"text-align: left;\">variety</th><th style = \"text-align: left;\">winery</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"InlineStrings.String3\" style = \"text-align: left;\">String3</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"InlineStrings.String15\" style = \"text-align: left;\">String15</th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"InlineStrings.String31\" style = \"text-align: left;\">String31</th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"String\" style = \"text-align: left;\">String</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">US</td><td style = \"text-align: right;\">87</td><td style = \"text-align: right;\">14.0</td><td style = \"text-align: left;\">Oregon</td><td style = \"text-align: left;\">Willamette Valley</td><td style = \"text-align: left;\">Willamette Valley</td><td style = \"text-align: left;\">Pinot Gris</td><td style = \"text-align: left;\">Rainstorm</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">US</td><td style = \"text-align: right;\">87</td><td style = \"text-align: right;\">65.0</td><td style = \"text-align: left;\">Oregon</td><td style = \"text-align: left;\">Willamette Valley</td><td style = \"text-align: left;\">Willamette Valley</td><td style = \"text-align: left;\">Pinot Noir</td><td style = \"text-align: left;\">Sweet Cheeks</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">US</td><td style = \"text-align: right;\">87</td><td style = \"text-align: right;\">19.0</td><td style = \"text-align: left;\">California</td><td style = \"text-align: left;\">Napa Valley</td><td style = \"text-align: left;\">Napa</td><td style = \"text-align: left;\">Cabernet Sauvignon</td><td style = \"text-align: left;\">Kirkland Signature</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">US</td><td style = \"text-align: right;\">87</td><td style = \"text-align: right;\">34.0</td><td style = \"text-align: left;\">California</td><td style = \"text-align: left;\">Alexander Valley</td><td style = \"text-align: left;\">Sonoma</td><td style = \"text-align: left;\">Cabernet Sauvignon</td><td style = \"text-align: left;\">Louis M. Martini</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">US</td><td style = \"text-align: right;\">87</td><td style = \"text-align: right;\">12.0</td><td style = \"text-align: left;\">California</td><td style = \"text-align: left;\">Central Coast</td><td style = \"text-align: left;\">Central Coast</td><td style = \"text-align: left;\">Chardonnay</td><td style = \"text-align: left;\">Mirassou</td></tr></tbody></table></div>","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"Sample 10,000 rows for faster computation (the full dataset is quite large):","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"df = df[sample(1:nrow(df), 10000, replace = false), :];","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"Coerce categorical columns to appropriate scientific types. We use autotype to automatically detect categorical features by recognizing columns with few unique values:","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"df = coerce(df, autotype(df, :few_to_finite));\ndf = coerce(df, :points => Continuous, :region_1 => Multiclass,\n    :variety => Multiclass, :winery => Multiclass);","category":"page"},{"location":"tutorials/wine_example/notebook/#Split-Data","page":"Wine Quality Prediction","title":"Split Data","text":"","category":"section"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"Separate features (X) from target (y), then split into train/test sets:","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"y, X = unpack(df, ==(:points); rng = 123);\ntrain, test = partition(eachindex(y), 0.8, shuffle = true, rng = 100);","category":"page"},{"location":"tutorials/wine_example/notebook/#Setup-Encoders-and-Model","page":"Wine Quality Prediction","title":"Setup Encoders and Model","text":"","category":"section"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"Load the required models and create different encoding strategies:","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"OneHot = @load OneHotEncoder pkg = MLJModels verbosity = 0\nCatBoostRegressor = @load CatBoostRegressor pkg = CatBoost","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"CatBoost.MLJCatBoostInterface.CatBoostRegressor","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"Encoding Strategies:","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"OneHotEncoder: Creates binary columns for each category\nFrequencyEncoder: Replaces categories with their frequency counts","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"In case of the one-hot-encoder, we worry when categories have high cardinality as that would lead to an explosion in the number of features.","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"card_reducer = MLJTransforms.CardinalityReducer(min_frequency = 10, ordered_factor = true)\nonehot_model = OneHot(drop_last = true, ordered_factor = true)\nfreq_model = MLJTransforms.FrequencyEncoder(normalize = false, ordered_factor = true)\ncat = CatBoostRegressor();","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"Create three different pipelines to compare:","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"pipelines = [\n    (\"CardRed + OneHot + CAT\", card_reducer |> onehot_model |> cat),\n    (\"OneHot + CAT\", onehot_model |> cat),\n    (\"FreqEnc + CAT\", freq_model |> cat),\n]","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"3-element Vector{Tuple{String, MLJBase.DeterministicPipeline{N, MLJModelInterface.predict} where N<:NamedTuple}}:\n (\"CardRed + OneHot + CAT\", DeterministicPipeline(cardinality_reducer = CardinalityReducer(features = Symbol[], …), …))\n (\"OneHot + CAT\", DeterministicPipeline(one_hot_encoder = OneHotEncoder(features = Symbol[], …), …))\n (\"FreqEnc + CAT\", DeterministicPipeline(frequency_encoder = FrequencyEncoder(features = Symbol[], …), …))","category":"page"},{"location":"tutorials/wine_example/notebook/#Evaluate-Pipelines","page":"Wine Quality Prediction","title":"Evaluate Pipelines","text":"","category":"section"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"Train each pipeline and measure both performance (RMSE) and training time:","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"results = DataFrame(pipeline = String[], rmse = Float64[], training_time = Float64[]);\n\nfor (name, pipe) in pipelines\n    println(\"Training: $name\")\n    mach = machine(pipe, X, y)\n    training_time = @elapsed MLJ.fit!(mach, rows = train)\n    predictions = MLJ.predict(mach, rows = test)\n    rmse_value = MLJ.root_mean_squared_error(y[test], predictions)\n    push!(results, (name, rmse_value, training_time))\nend","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"Training: CardRed + OneHot + CAT\n[ Info: Training machine(DeterministicPipeline(cardinality_reducer = CardinalityReducer(features = Symbol[], …), …), …).\n[ Info: Training machine(:cardinality_reducer, …).\n[ Info: Training machine(:one_hot_encoder, …).\n[ Info: Spawning 0 sub-features to one-hot encode feature :country.\n[ Info: Spawning 3 sub-features to one-hot encode feature :province.\n[ Info: Spawning 88 sub-features to one-hot encode feature :region_1.\n[ Info: Spawning 16 sub-features to one-hot encode feature :region_2.\n[ Info: Spawning 48 sub-features to one-hot encode feature :variety.\n[ Info: Spawning 408 sub-features to one-hot encode feature :winery.\n[ Info: Training machine(:cat_boost_regressor, …).\nTraining: OneHot + CAT\n[ Info: Training machine(DeterministicPipeline(one_hot_encoder = OneHotEncoder(features = Symbol[], …), …), …).\n[ Info: Training machine(:one_hot_encoder, …).\n[ Info: Spawning 0 sub-features to one-hot encode feature :country.\n[ Info: Spawning 3 sub-features to one-hot encode feature :province.\n[ Info: Spawning 157 sub-features to one-hot encode feature :region_1.\n[ Info: Spawning 16 sub-features to one-hot encode feature :region_2.\n[ Info: Spawning 148 sub-features to one-hot encode feature :variety.\n[ Info: Spawning 2986 sub-features to one-hot encode feature :winery.\n[ Info: Training machine(:cat_boost_regressor, …).\nTraining: FreqEnc + CAT\n[ Info: Training machine(DeterministicPipeline(frequency_encoder = FrequencyEncoder(features = Symbol[], …), …), …).\n[ Info: Training machine(:frequency_encoder, …).\n[ Info: Training machine(:cat_boost_regressor, …).\n","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"Sort by RMSE (lower is better) and display results:","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"sort!(results, :rmse)\nresults","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"<div><div style = \"float: left;\"><span>3×3 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">pipeline</th><th style = \"text-align: left;\">rmse</th><th style = \"text-align: left;\">training_time</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">OneHot + CAT</td><td style = \"text-align: right;\">2.3801</td><td style = \"text-align: right;\">148.974</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">CardRed + OneHot + CAT</td><td style = \"text-align: right;\">2.38979</td><td style = \"text-align: right;\">93.2509</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">FreqEnc + CAT</td><td style = \"text-align: right;\">2.39178</td><td style = \"text-align: right;\">32.5362</td></tr></tbody></table></div>","category":"page"},{"location":"tutorials/wine_example/notebook/#Visualization","page":"Wine Quality Prediction","title":"Visualization","text":"","category":"section"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"Create side-by-side bar charts to compare both training time and model performance:","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"n = nrow(results)\n\ntime_plot = bar(1:n, results.training_time;\n    xticks = (1:n, results.pipeline), title = \"Training Time (seconds)\",\n    xlabel = \"Pipeline\", ylabel = \"Time (s)\", xrotation = 45,\n    legend = false, color = :lightblue);\n\nrmse_plot = bar(1:n, results.rmse;\n    xticks = (1:n, results.pipeline), title = \"Root Mean Squared Error\",\n    xlabel = \"Pipeline\", ylabel = \"RMSE\", xrotation = 45,\n    legend = false, color = :lightcoral);\n\ncombined_plot = plot(time_plot, rmse_plot; layout = (1, 2), size = (1200, 500));","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"Save the plot","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"(Image: Wine Encoding Comparison)","category":"page"},{"location":"tutorials/wine_example/notebook/#Conclusion","page":"Wine Quality Prediction","title":"Conclusion","text":"","category":"section"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"Key Findings:","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"The model performance did not vary significantly across encoding strategies.\nWe observe a decent speed up in using the cardinality reducer before one-hot encoding with close to no impact on performance.\nThat said, frequency encoder led to the least training time as it didn't add any new features","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"","category":"page"},{"location":"tutorials/wine_example/notebook/","page":"Wine Quality Prediction","title":"Wine Quality Prediction","text":"This page was generated using Literate.jl.","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"EditURL = \"notebook.jl\"","category":"page"},{"location":"tutorials/classic_comparison/notebook/#Categorical-Encoders-Performance:-A-Classic-Comparison","page":"Milk Quality Classification","title":"Categorical Encoders Performance: A Classic Comparison","text":"","category":"section"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"Julia version is assumed to be 1.10.*","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"This demonstration is available as a Jupyter notebook or julia script (as well as the dataset) here.","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"This tutorial compares four fundamental categorical encoding approaches on a milk quality dataset: OneHot, Frequency, Target, and Ordinal encoders paired with SVM classification.","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"using Pkg;\nPkg.activate(@__DIR__);\n\nusing MLJ, MLJTransforms, LIBSVM, DataFrames, ScientificTypes\nusing Random, CSV","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"  Activating project at `~/Documents/GitHub/MLJTransforms/docs/src/tutorials/classic_comparison`\n","category":"page"},{"location":"tutorials/classic_comparison/notebook/#Load-and-Prepare-Data","page":"Milk Quality Classification","title":"Load and Prepare Data","text":"","category":"section"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"Load the milk quality dataset which contains categorical features for quality prediction:","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"df = CSV.read(\"./milknew.csv\", DataFrame)\n\nfirst(df, 5)","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"<div><div style = \"float: left;\"><span>5×8 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">pH</th><th style = \"text-align: left;\">Temprature</th><th style = \"text-align: left;\">Taste</th><th style = \"text-align: left;\">Odor</th><th style = \"text-align: left;\">Fat </th><th style = \"text-align: left;\">Turbidity</th><th style = \"text-align: left;\">Colour</th><th style = \"text-align: left;\">Grade</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"InlineStrings.String7\" style = \"text-align: left;\">String7</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">6.6</td><td style = \"text-align: right;\">35</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">254</td><td style = \"text-align: left;\">high</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">6.6</td><td style = \"text-align: right;\">36</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">253</td><td style = \"text-align: left;\">high</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">8.5</td><td style = \"text-align: right;\">70</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">246</td><td style = \"text-align: left;\">low</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">9.5</td><td style = \"text-align: right;\">34</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">255</td><td style = \"text-align: left;\">low</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">6.6</td><td style = \"text-align: right;\">37</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">255</td><td style = \"text-align: left;\">medium</td></tr></tbody></table></div>","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"Check the scientific types to understand our data structure:","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"ScientificTypes.schema(df)","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"┌────────────┬────────────┬─────────┐\n│ names      │ scitypes   │ types   │\n├────────────┼────────────┼─────────┤\n│ pH         │ Continuous │ Float64 │\n│ Temprature │ Count      │ Int64   │\n│ Taste      │ Count      │ Int64   │\n│ Odor       │ Count      │ Int64   │\n│ Fat        │ Count      │ Int64   │\n│ Turbidity  │ Count      │ Int64   │\n│ Colour     │ Count      │ Int64   │\n│ Grade      │ Textual    │ String7 │\n└────────────┴────────────┴─────────┘\n","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"Automatically coerce columns with few unique values to categorical:","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"df = coerce(df, autotype(df, :few_to_finite))\n\nScientificTypes.schema(df)","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"┌────────────┬───────────────────┬───────────────────────────────────┐\n│ names      │ scitypes          │ types                             │\n├────────────┼───────────────────┼───────────────────────────────────┤\n│ pH         │ OrderedFactor{16} │ CategoricalValue{Float64, UInt32} │\n│ Temprature │ OrderedFactor{17} │ CategoricalValue{Int64, UInt32}   │\n│ Taste      │ OrderedFactor{2}  │ CategoricalValue{Int64, UInt32}   │\n│ Odor       │ OrderedFactor{2}  │ CategoricalValue{Int64, UInt32}   │\n│ Fat        │ OrderedFactor{2}  │ CategoricalValue{Int64, UInt32}   │\n│ Turbidity  │ OrderedFactor{2}  │ CategoricalValue{Int64, UInt32}   │\n│ Colour     │ OrderedFactor{9}  │ CategoricalValue{Int64, UInt32}   │\n│ Grade      │ Multiclass{3}     │ CategoricalValue{String7, UInt32} │\n└────────────┴───────────────────┴───────────────────────────────────┘\n","category":"page"},{"location":"tutorials/classic_comparison/notebook/#Split-Data","page":"Milk Quality Classification","title":"Split Data","text":"","category":"section"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"Separate features from target and create train/test split:","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"y, X = unpack(df, ==(:Grade); rng = 123)\ntrain, test = partition(eachindex(y), 0.9, shuffle = true, rng = 100);","category":"page"},{"location":"tutorials/classic_comparison/notebook/#Setup-Encoders-and-Classifier","page":"Milk Quality Classification","title":"Setup Encoders and Classifier","text":"","category":"section"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"Load the required models and create different encoding strategies:","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"OneHot = @load OneHotEncoder pkg = MLJModels verbosity = 0\nSVC = @load SVC pkg = LIBSVM verbosity = 0","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"MLJLIBSVMInterface.SVC","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"Encoding Strategies Explained:","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"OneHot: Creates binary columns for each category (sparse, interpretable)\nFrequency: Replaces categories with their occurrence frequency\nTarget: Uses target statistics for each category\nOrdinal: Assigns integer codes to categories (assumes ordering)","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"onehot_model = OneHot(drop_last = true, ordered_factor = true)\nfreq_model = MLJTransforms.FrequencyEncoder(normalize = false, ordered_factor = true)\ntarget_model = MLJTransforms.TargetEncoder(lambda = 0.9, m = 5, ordered_factor = true)\nordinal_model = MLJTransforms.OrdinalEncoder(ordered_factor = true)\nsvm = SVC()","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"SVC(\n  kernel = LIBSVM.Kernel.RadialBasis, \n  gamma = 0.0, \n  cost = 1.0, \n  cachesize = 200.0, \n  degree = 3, \n  coef0 = 0.0, \n  tolerance = 0.001, \n  shrinking = true)","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"Create four different pipelines to compare:","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"pipelines = [\n    (\"OneHot + SVM\", onehot_model |> svm),\n    (\"FreqEnc + SVM\", freq_model |> svm),\n    (\"TargetEnc + SVM\", target_model |> svm),\n    (\"Ordinal + SVM\", ordinal_model |> svm),\n]","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"4-element Vector{Tuple{String, MLJBase.DeterministicPipeline{N, MLJModelInterface.predict} where N<:NamedTuple}}:\n (\"OneHot + SVM\", DeterministicPipeline(one_hot_encoder = OneHotEncoder(features = Symbol[], …), …))\n (\"FreqEnc + SVM\", DeterministicPipeline(frequency_encoder = FrequencyEncoder(features = Symbol[], …), …))\n (\"TargetEnc + SVM\", DeterministicPipeline(target_encoder = TargetEncoder(features = Symbol[], …), …))\n (\"Ordinal + SVM\", DeterministicPipeline(ordinal_encoder = OrdinalEncoder(features = Symbol[], …), …))","category":"page"},{"location":"tutorials/classic_comparison/notebook/#Evaluate-Pipelines","page":"Milk Quality Classification","title":"Evaluate Pipelines","text":"","category":"section"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"Use 10-fold cross-validation to robustly estimate each pipeline's accuracy:","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"results = DataFrame(pipeline = String[], accuracy = Float64[])\n\nfor (name, pipe) in pipelines\n    println(\"Evaluating: $name\")\n    mach = machine(pipe, X, y)\n    eval_results = evaluate!(\n        mach,\n        resampling = CV(nfolds = 10, rng = 123),\n        measure = accuracy,\n        rows = train,\n        verbosity = 0,\n    )\n    acc = mean(eval_results.measurement)\n    push!(results, (name, acc))\nend","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"Evaluating: OneHot + SVM\nEvaluating: FreqEnc + SVM\nEvaluating: TargetEnc + SVM\nEvaluating: Ordinal + SVM\n","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"Sort results by accuracy (highest first) and display:","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"sort!(results, :accuracy, rev = true)\nresults","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"<div><div style = \"float: left;\"><span>4×2 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">pipeline</th><th style = \"text-align: left;\">accuracy</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String\" style = \"text-align: left;\">String</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">OneHot + SVM</td><td style = \"text-align: right;\">0.998951</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">TargetEnc + SVM</td><td style = \"text-align: right;\">0.974816</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">Ordinal + SVM</td><td style = \"text-align: right;\">0.940189</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">FreqEnc + SVM</td><td style = \"text-align: right;\">0.885624</td></tr></tbody></table></div>","category":"page"},{"location":"tutorials/classic_comparison/notebook/#Results-Analysis","page":"Milk Quality Classification","title":"Results Analysis","text":"","category":"section"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"We notice that one-hot-encoding was the most performant here followed by target encoding. Ordinal encoding also produced decent results because we can perceive all the categorical variables to be ordered On the other hand, frequency encoding lagged behind. Observe that this method doesn't distinguish categories from one another if they occur with similar frequencies.","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"","category":"page"},{"location":"tutorials/classic_comparison/notebook/","page":"Milk Quality Classification","title":"Milk Quality Classification","text":"This page was generated using Literate.jl.","category":"page"},{"location":"transformers/contrast/","page":"Contrast Encoders","title":"Contrast Encoders","text":"Contrast Encoders include categorical encoders that could be modeled by a contrast matrix:","category":"page"},{"location":"transformers/contrast/","page":"Contrast Encoders","title":"Contrast Encoders","text":"Transformer Brief Description\nDummyEncoder Encodes by comparing each level to the reference level, intercept being the cell mean of the reference group\nSumEncoder Encodes by comparing each level to the reference level, intercept being the grand mean\nHelmertEncoder Encodes by comparing levels of a variable with the mean of the subsequent levels of the variable\nForwardDifferenceEncoder Encodes by comparing adjacent levels of a variable (each level minus the next level)\nContrastEncoder Allows defining a custom contrast encoder via a contrast matrix\nHypothesisEncoder Allows defining a custom contrast encoder via a hypothesis matrix","category":"page"},{"location":"transformers/contrast/","page":"Contrast Encoders","title":"Contrast Encoders","text":"MLJTransforms.ContrastEncoder","category":"page"},{"location":"transformers/contrast/#MLJTransforms.ContrastEncoder","page":"Contrast Encoders","title":"MLJTransforms.ContrastEncoder","text":"ContrastEncoder\n\nA model type for constructing a contrast encoder, based on MLJTransforms.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nContrastEncoder = @load ContrastEncoder pkg=MLJTransforms\n\nDo model = ContrastEncoder() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in ContrastEncoder(features=...).\n\nContrastEncoder implements the following contrast encoding methods for  categorical features: dummy, sum, backward/forward difference, and Helmert coding.  More generally, users can specify a custom contrast or hypothesis matrix, and each feature  can be encoded using a different method.\n\nTraining data\n\nIn MLJ (or MLJBase) bind an instance unsupervised model to data with\n\nmach = machine(model, X)\n\nHere:\n\nX is any table of input features (eg, a DataFrame). Features to be transformed must  have element scitype Multiclass or OrderedFactor. Use schema(X) to   check scitypes. \n\nTrain the machine using fit!(mach, rows=...).\n\nHyper-parameters\n\nfeatures=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of ignore, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\nmode=:dummy: The type of encoding to use. Can be one of :contrast, :dummy, :sum, :backward_diff, :forward_diff, :helmert or :hypothesis.\n\nIf ignore=false (features to be encoded are listed explictly in features), then this can be a vector of the same length as features to specify a different contrast encoding scheme for each feature\n\nbuildmatrix=nothing: A function or other callable with signature buildmatrix(colname, k), \n\nwhere colname is the name of the feature levels and k is it's length, and which returns contrast or  hypothesis matrix with row/column ordering consistent with the ordering of levels(col). Only relevant if mode is :contrast or :hypothesis.\n\nignore=true: Whether to exclude or include the features given in features\nordered_factor=false: Whether to encode OrderedFactor or ignore them\n\nOperations\n\ntransform(mach, Xnew): Apply contrast encoding to selected Multiclass or OrderedFactor features of Xnew specified by hyper-parameters, and   return the new table. Features that are neither Multiclass nor OrderedFactor  are always left unchanged.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nvector_given_value_given_feature: A dictionary that maps each level for each column in a subset of the categorical features of X into its frequency.\n\nReport\n\nThe fields of report(mach) are:\n\nencoded_features: The subset of the categorical features of X that were encoded\n\nExamples\n\nusing MLJ\n\n# Define categorical dataset\nX = (\n    name   = categorical([\"Ben\", \"John\", \"Mary\", \"John\"]),\n    height = [1.85, 1.67, 1.5, 1.67],\n    favnum = categorical([7, 5, 10, 1]),\n    age    = [23, 23, 14, 23],\n)\n\n# Check scitype coercions:\nschema(X)\n\nencoder =  ContrastEncoder(\n    features = [:name, :favnum],\n    ignore = false, \n    mode = [:dummy, :helmert],\n)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia > Xnew\n    (name_John = [1.0, 0.0, 0.0, 0.0],\n    name_Mary = [0.0, 1.0, 0.0, 1.0],\n    height = [1.85, 1.67, 1.5, 1.67],\n    favnum_5 = [0.0, 1.0, 0.0, -1.0],\n    favnum_7 = [2.0, -1.0, 0.0, -1.0],\n    favnum_10 = [-1.0, -1.0, 3.0, -1.0],\n    age = [23, 23, 14, 23],)\n\nSee also OneHotEncoder\n\n\n\n\n\n","category":"type"},{"location":"tutorials/T1/","page":"-","title":"-","text":"This is an imaginary tutorial.","category":"page"},{"location":"#MLJTransforms.jl","page":"Introduction","title":"MLJTransforms.jl","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"A Julia package providing a wide range of categorical encoders and transformers to be used with the MLJ package. Transformers help convert raw features into a representation that's better suited for downstream models. Meanwhile, categorical encoders are a type of transformer that specifically encodes categorical features into numerical forms. ","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"import Pkg\nPkg.activate(\"my_environment\", shared=true)\nPkg.add([\"MLJ\", \"MLJTransforms\"])","category":"page"},{"location":"#Quick-Start","page":"Introduction","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"For the following demo, you will need to additionally run Pkg.add(\"RDatasets\").","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using MLJ\nimport RDatasets\n\n# 1. Load Data\nX = RDatasets.dataset(\"HSAUR\", \"Forbes2000\");\n\n# 2. Load the model\nFrequencyEncoder = @load FrequencyEncoder pkg=\"MLJTransforms\"\nencoder = FrequencyEncoder(\n    features=[:Country, :Category],     # The categorical columns to select\n    ignore=false,                       # Whether to exclude or include selected columns\n    ordered_factor = false,             # Whether to also encode columns of ordered factor elements\n    normalize=true                      # Whether to normalize the frequencies used for encoding\n    )\n\n\n# 3. Wrap it in a machine and fit\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)","category":"page"},{"location":"#Available-Transformers","page":"Introduction","title":"Available Transformers","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"See complete list of transformers in this package.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"In MLJTransforms we denote transformers that can operate on columns with Continuous and/or Count scientific types as numerical transformers. Meanwhile, categorical transformers operate on Multiclass and/or OrderedFactor scientific types. Most categorical transformers in this package operate by converting categorical values into numerical values or vectors, and are therefore considered categorical encoders. We categorize categorical encoders as follows:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Category Description\nClassical Encoders Traditional categorical encoding algorithms and techniques.\nNeural-based Encoders Categorical encoders based on neural networks.\nContrast Encoders Categorical encoders that could be modeled via a contrast matrix.\nUtility Encoders Categorical encoders meant to be used as preprocessors for other transformers or models.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Some transformers in this package can even operate on both Finite and Infinite scientific types or other special scientific types (eg, to represent time). To learn more about scientific types see the official documentation.","category":"page"}]
}
