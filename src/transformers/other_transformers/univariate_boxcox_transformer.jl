
function standardize(v)
    map(v) do x
        (x - mean(v))/std(v)
    end
end

function midpoints(v::AbstractVector{T}) where T <: Real
    return [0.5*(v[i] + v[i + 1]) for i in 1:(length(v) -1)]
end

function normality(v)
    n  = length(v)
    v  = standardize(convert(Vector{Float64}, v))
    # sort and replace with midpoints
    v = midpoints(sort!(v))
    # find the (approximate) expected value of the size (n-1)-ordered statistics for
    # standard normal:
    d = Distributions.Normal(0,1)
    w = map(collect(1:(n-1))/n) do x
        quantile(d, x)
    end
    return cor(v, w)
end

function boxcox(lambda, c, x::Real)
    c + x >= 0 || throw(DomainError)
    if lambda == 0.0
        c + x > 0 || throw(DomainError)
        return log(c + x)
    end
    return ((c + x)^lambda - 1)/lambda
end

boxcox(lambda, c, v::AbstractVector{T}) where T <: Real =
    [boxcox(lambda, c, x) for x in v]

@with_kw_noshow mutable struct UnivariateBoxCoxTransformer <: Unsupervised
    n::Int      = 171   # nbr values tried in optimizing exponent lambda
    shift::Bool = false # whether to shift data away from zero
end

function MMI.fit(transformer::UnivariateBoxCoxTransformer, verbosity::Int,
             v::AbstractVector{T}) where T <: Real

    m = minimum(v)
    m >= 0 || error("Cannot perform a Box-Cox transformation on negative data.")

    c = 0.0 # default
    if transformer.shift
        if m == 0
            c = 0.2*mean(v)
        end
    else
        m != 0 || error("Zero value encountered in data being Box-Cox transformed.\n"*
                        "Consider calling `fit!` with `shift=true`.")
    end

    lambdas = range(-0.4, stop=3, length=transformer.n)
    scores = Float64[normality(boxcox(l, c, v)) for l in lambdas]
    lambda = lambdas[argmax(scores)]

    return  (lambda, c), nothing, NamedTuple()
end

MMI.fitted_params(::UnivariateBoxCoxTransformer, fitresult) =
    (λ=fitresult[1], c=fitresult[2])

# for X scalar or vector:
MMI.transform(transformer::UnivariateBoxCoxTransformer, fitresult, X) =
    boxcox(fitresult..., X)

# scalar case:
function MMI.inverse_transform(transformer::UnivariateBoxCoxTransformer,
                           fitresult, x::Real)
    lambda, c = fitresult
    if lambda == 0
        return exp(x) - c
    else
        return (lambda*x + 1)^(1/lambda) - c
    end
end

# vector case:
function MMI.inverse_transform(transformer::UnivariateBoxCoxTransformer,
                           fitresult, w::AbstractVector{T}) where T <: Real
    return [inverse_transform(transformer, fitresult, y) for y in w]
end

metadata_model(UnivariateBoxCoxTransformer,
    input_scitype   = AbstractVector{Continuous},
    output_scitype = AbstractVector{Continuous},
    human_name = "single variable Box-Cox transformer",
    load_path = "MLJTransforms.UnivariateBoxCoxTransformer")

"""
$(MLJModelInterface.doc_header(UnivariateBoxCoxTransformer))

Box-Cox transformations attempt to make data look more normally
distributed. This can improve performance and assist in the
interpretation of models which suppose that data is
generated by a normal distribution.

A Box-Cox transformation (with shift) is of the form

    x -> ((x + c)^λ - 1)/λ

for some constant `c` and real `λ`, unless `λ = 0`, in which
case the above is replaced with

    x -> log(x + c)

Given user-specified hyper-parameters `n::Integer` and `shift::Bool`,
the present implementation learns the parameters `c` and `λ` from the
training data as follows: If `shift=true` and zeros are encountered in
the data, then `c` is set to `0.2` times the data mean.  If there are
no zeros, then no shift is applied. Finally, `n` different values of `λ`
between `-0.4` and `3` are considered, with `λ` fixed to the value
maximizing normality of the transformed data.

*Reference:* [Wikipedia entry for power
 transform](https://en.wikipedia.org/wiki/Power_transform).


# Training data

In MLJ or MLJBase, bind an instance `model` to data with

    mach = machine(model, x)

where

- `x`: any abstract vector with element scitype `Continuous`; check
  the scitype with `scitype(x)`

Train the machine using `fit!(mach, rows=...)`.


# Hyper-parameters

- `n=171`: number of values of the exponent `λ` to try

- `shift=false`: whether to include a preliminary constant translation
  in transformations, in the presence of zeros


# Operations

- `transform(mach, xnew)`: apply the Box-Cox transformation learned when fitting `mach`

- `inverse_transform(mach, z)`: reconstruct the vector `z` whose
  transformation learned by `mach` is `z`


# Fitted parameters

The fields of `fitted_params(mach)` are:

- `λ`: the learned Box-Cox exponent

- `c`: the learned shift


# Examples

```
using MLJ
using UnicodePlots
using Random
Random.seed!(123)

transf = UnivariateBoxCoxTransformer()

x = randn(1000).^2

mach = machine(transf, x)
fit!(mach)

z = transform(mach, x)

julia> histogram(x)
                ┌                                        ┐
   [ 0.0,  2.0) ┤███████████████████████████████████  848
   [ 2.0,  4.0) ┤████▌ 109
   [ 4.0,  6.0) ┤█▍ 33
   [ 6.0,  8.0) ┤▍ 7
   [ 8.0, 10.0) ┤▏ 2
   [10.0, 12.0) ┤  0
   [12.0, 14.0) ┤▏ 1
                └                                        ┘
                                 Frequency

julia> histogram(z)
                ┌                                        ┐
   [-5.0, -4.0) ┤█▎ 8
   [-4.0, -3.0) ┤████████▊ 64
   [-3.0, -2.0) ┤█████████████████████▊ 159
   [-2.0, -1.0) ┤█████████████████████████████▊ 216
   [-1.0,  0.0) ┤███████████████████████████████████  254
   [ 0.0,  1.0) ┤█████████████████████████▊ 188
   [ 1.0,  2.0) ┤████████████▍ 90
   [ 2.0,  3.0) ┤██▊ 20
   [ 3.0,  4.0) ┤▎ 1
                └                                        ┘
                                 Frequency

```

"""
UnivariateBoxCoxTransformer